% =========================================================
% قالب تخرج جامعي مبني على كتيّب التعليمات
% يدعم Times New Roman عبر XeLaTeX + fontspec
% =========================================================

\documentclass[12pt,a4paper]{report} % نستخدم report لتوفير chapters (فصول)

% ------------------------- الخطوط واللغات -------------------------
% نستخدم XeLaTeX لضبط Times New Roman الحقيقي
\usepackage{fontspec}            % تحميل خطوط النظام
\setmainfont{Times New Roman}    % الخط الأساسي للنص والعناوين (حسب التعليمات)
% في حال لم يكن الخط مثبتاً، ثبّت Times New Roman أو استبدله بـ "Times New Roman PS MT"
% \setmainfont{Times New Roman PS MT}



% ------------------------- الهوامش والمسافات -----------------------
\usepackage[a4paper,margin=2.5cm]{geometry} % هوامش A4 قياسية
\usepackage{setspace}
\usepackage[bottom]{footmisc} % الحاشية السفلية
\onehalfspacing % مسافة أسطر 1.5 كما يُفضَّل عادةً للتقارير
\usepackage{fancyhdr}



% ------------------------- رسومات/جداول/عناوين ---------------------
\usepackage{graphicx}            % إدراج الصور
\usepackage{caption}             % (يجب أن تُحمَّل قبل polyglossia/bidi لتفادي الخطأ)
\usepackage{booktabs}            % جمال الجداول
\usepackage{array}               % أعمدة p{...} وتوسعة خيارات الجداول
% نوع عمود p{} لكن مُتمركز + يسمح بـ \\ في نهاية الخلية
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{mdframed}            % لإطار العناوين (يجب قبل polyglossia/bidi)
\usepackage{tocloft}             % للتحكم بشكل الفهارس
\usepackage{enumitem}            % قوائم بنقاط مرتبة وتخصيص المسافات
% تحسين نقط القادة (dots)
\usepackage{float}
\renewcommand{\cftdotsep}{1}      % تقارب النقاط
\setlength{\cftbeforechapskip}{6pt}
\setlength{\cftbeforesecskip}{2pt}
\usepackage{comment}
% ------------------------- Code Listings -------------------------
\usepackage{xcolor}
\usepackage{listings}
\usepackage{longtable}

% Listings style for Python/YAML/Bash
\lstset{
  basicstyle=\small\ttfamily,
  keywordstyle=\color{blue}\bfseries,
  commentstyle=\color{gray}\itshape,
  stringstyle=\color{orange},
  showstringspaces=false,
  breaklines=true,
  frame=single,
  numbers=left,
  numberstyle=\tiny\color{gray},
  numbersep=5pt,
  xleftmargin=15pt,
  tabsize=4,
  captionpos=b
}
\lstdefinestyle{python}{language=Python}
\lstdefinestyle{yaml}{language=C}  % Approximate for YAML
\lstdefinestyle{bash}{language=bash}
% --------- إعادة تعريف \chapter ليكون غلاف الفصل ---------
\renewcommand{\chapter}[1]{%
  \cleardoublepage
  \refstepcounter{chapter}% زيادة عداد الفصل
  \phantomsection
  \addcontentsline{toc}{chapter}{\protect\numberline{\thechapter}#1}% إدخال في الفهرس
  \thispagestyle{empty}% بدون رأس/تذييل

  \begin{center}
    \vspace*{0.3\textheight}
    {\bfseries\itshape\fontsize{36}{40}\selectfont Chapter \thechapter}\\[1.5cm]
    {\bfseries\fontsize{36}{40}\selectfont #1}
  \end{center}

  % إعادة ضبط ترقيم الأقسام
  \setcounter{section}{0}%
  \setcounter{subsection}{0}%
  \setcounter{subsubsection}{0}%

  \clearpage
}


%---------------- بيئة صندوق تشمل العنوان + المحتوى (mdframed بمفاتيح صحيحة)-----------
\newenvironment{boxedlist}[1]{%
  \begin{mdframed}[
    linewidth=0.8pt,
    roundcorner=2pt,
    skipabove=10pt,
    skipbelow=14pt,
    innerleftmargin=14pt, innerrightmargin=14pt,
    innertopmargin=8pt, innerbottommargin=12pt,
    frametitle={\Large\bfseries #1},
    frametitlealignment=\center,
    frametitleaboveskip=2pt,
    frametitlebelowskip=8pt
  ]%
}{%
  \end{mdframed}%
}


% ------------------------- الترميز والروابط ------------------------
% (hyperref يجب أن يحمَّل أيضاً قبل polyglossia/bidi لتفادي تحذيرات bidi)
\usepackage{hyperref} % روابط داخلية وخارجية
\hypersetup{
  colorlinks=true,
  linkcolor=black,
  urlcolor=blue,
  pdftitle={Graduation Project},
  pdfauthor={Student Name},
  pdfsubject={Graduation Project Report}
}

% ------------------------- اللغات (ثنائي الاتجاه) -----------------
% دعم العربية والإنجليزية (اختياري لكن مفيد لو كتبت بالعربية داخل المستند)
% ملاحظة: تحميل polyglossia بعد كل الحزم أعلاه حتى لا يشتكي bidi.
\usepackage{polyglossia}
\setdefaultlanguage{english}     % لغة المستند الأساسية (كما في النموذج الإنجليزي)
\setotherlanguage{arabic}        % لغة إضافية للعربية عند الحاجة
\newfontfamily\arabicfont[Script=Arabic]{Times New Roman} % توحيد الخط أيضاً للنص العربي

% ------------------------- الترقيم والعناوين -----------------------
% report يرقم: Chapter 1, ثم Section 1.1, ثم Subsection 1.1.1 تلقائياً
\setcounter{secnumdepth}{3} % ترقيم حتى subsubsection
\setcounter{tocdepth}{3}    % إظهار حتى subsubsection في جدول المحتويات

% (اختياري) تحكم أدق بشكل أرقام العناوين لو احتجت:
% \renewcommand\thesection{\thechapter.\arabic{section}}
% \renewcommand\thesubsection{\thesection.\arabic{subsection}}
% \renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}

% ------------------------- أدوات مرجعية (اختياري) ------------------
% \usepackage[style=ieee]{biblatex}
% \addbibresource{references.bib}


% =========================================================
%                 بداية المستند
% =========================================================

\begin{document}

% ------------------------- صفحة الغلاف ----------------------------
% التعليمات تحدد: العنوان Times New Roman حجم ~20 Bold،
% أسماء الطلاب والخ، وخط الأساس 12pt لباقي الصفحات. (المقاسات هنا قريبة جداً للمذكور)
\begin{titlepage}
  \centering

 \begin{minipage}[t]{0.62\textwidth}\vspace{0pt}
    \raggedright
    \textbf{Republic of Yemen}\\
    Ministry of Higher Education and\\
    Scientific Research\\
    University of Science and Technology\\
    Faculty of Computing and Information Technology
\end{minipage}%
\hfill
\begin{minipage}[t]{0.28\textwidth}\vspace{0pt}
    \raggedleft
    \includegraphics[height=2.7cm]{images/UST-logo.png}
\end{minipage}

\vspace{2.8cm}

  % عنوان المشروع بحجم كبير وبولد (قريب من 20pt)
  {\fontsize{20pt}{22pt}\selectfont \textbf{Analytical Intelligence AI: Ubuntu Server Intelligence Log Collection and Analysis Tool}}\\[1.5cm]

  % أسماء الطلاب وأرقامهم (تقريبًا 16pt بحسب الإرشادات)
  {\fontsize{16pt}{18pt}\selectfont
  Amr Khaled AL-Awadhi \hfill 202210400203\\
  Nezar Faris AL-Hammadi \hfill Student No.\\
  Haitham Adnan AL-Shawafi \hfill Student No.\\
  Abdulrahman Abdulmalek AL-Halqi \hfill Student No.\\
  [2.0cm]
  }

  % المشرف
  {\fontsize{16pt}{18pt}\selectfont \textbf{Supervised by}}\\[0.5cm]
  {\fontsize{16pt}{18pt}\selectfont Dr. Wedad AL-Sorori}\\
  {\fontsize{14pt}{16pt}\selectfont Lecturer at the University of Science and Technology}\\[3.5cm]

  % وصف مختصر أسفل الصفحة + السنة
  {\normalsize
  A graduation project document submitted to the Department of Information Technology in partial\\
  fulfillment of the requirements for Bachelor’s degree in Cybersecurity}\\[1.5cm]

  {\Large \textbf{2025}\par}

\end{titlepage}

% ====================== FRONT MATTER (ROMAN) ======================
\clearpage
\pagenumbering{Roman}


% ----------------------------- Summary -----------------------------
\addcontentsline{toc}{chapter}{Summary}
\begin{mdframed}[
  linewidth=0.8pt,
  roundcorner=2pt,
  skipabove=10pt,
  skipbelow=14pt,
  innerleftmargin=14pt,
  innerrightmargin=14pt,
  innertopmargin=25pt,   % ← بدلاً من 8pt
  innerbottommargin=25pt
]


  % العنوان في الوسط
  \begin{center}
    {\bfseries\large Summary}
  \end{center}


  % النص الأساسي (حجم 12 في مستند 12pt)
  \vspace{6pt}
  {\normalsize
  The summary of the graduation project document should not exceed one page.
  The summary should contain the following:
  }

  % نقاط المحتوى كما بالصورة
  \vspace{4pt}
  \begin{itemize}[left=16pt,itemsep=2pt,topsep=4pt]
    \item Introduction statement
    \item Problem definition
    \item Main objectives of the graduation project
    \item SW/HW Tools
    \item Results
    \item Conclusions and recommendations (in short)
  \end{itemize}
\end{mdframed}

% --------------------------- Authorization --------------------------
\addcontentsline{toc}{chapter}{Authorization}
\begin{mdframed}[
  linewidth=0.8pt,
  roundcorner=2pt,
  skipabove=10pt,
  skipbelow=14pt,
  innerleftmargin=14pt,
  innerrightmargin=14pt,
  innertopmargin=25pt,
  innerbottommargin=25pt
]
  \begin{center}
    {\bfseries\large Authorization}
  \end{center}

  {\normalsize
  We authorize University of \dotfill\ Faculty of \dotfill\ to supply copies of our
  graduation project report to libraries, organizations or individuals on request.
  }

  \vspace{8pt}

% جدول مُحاط بخطوط عمودية وأفقية، كل الأعمدة مُتمركزة
\renewcommand{\arraystretch}{1.3} % ارتفاع صفوف أريح
\begin{tabular}{|C{0.40\linewidth}|C{0.23\linewidth}|C{0.17\linewidth}|}
  \hline
  \textbf{Student Name} & \textbf{Student No.} & \textbf{Signature} \\
  \hline
  % صفوف فارغة للتوقيع (أضف \rule لارتفاع بسيط)
  \rule{0pt}{2.2ex} & & \\ \hline
  \rule{0pt}{2.2ex} & & \\ \hline
  \rule{0pt}{2.2ex} & & \\ \hline
\end{tabular}

  \vspace{8pt}

  {\normalsize Date: \dotfill}
\end{mdframed}

% ----------------------------- Dedication ---------------------------
\addcontentsline{toc}{chapter}{Dedication}
\begin{mdframed}[
  linewidth=0.8pt,
  roundcorner=2pt,
  skipabove=10pt,
  skipbelow=14pt,
  innerleftmargin=14pt,
  innerrightmargin=14pt,
  innertopmargin=25pt,
  innerbottommargin=25pt
]
  \begin{center}
    {\bfseries\large Dedication}
  \end{center}

  % سطر التوضيح كما في الصورة
  {\normalsize This page is optional. The students can dedicate their project in this page.}

  \vspace{8pt}
  \textbf{Example}

  \vspace{2pt}
  {\normalsize
  To our families who made this achievement possible.
  }
\end{mdframed}


% ------------------------- Acknowledgment --------------------------
\clearpage
\addcontentsline{toc}{chapter}{Acknowledgment}
\begin{boxedlist}{Acknowledgment}
  % اكتب الشكر وفق الصيغة المقترحة في الكتيّب
  We would like to express our deepest gratitude to our families, colleagues, and instructors
  who provided continuous support and guidance throughout the preparation of this graduation project.
\end{boxedlist}

% ------------------------- Supervisor Certification ----------------
\clearpage
\addcontentsline{toc}{chapter}{Supervisor Certification}
\begin{boxedlist}{Supervisor Certification}
  We certify that the preparation of this project entitled ``\dots'' prepared by \dots\ 
  was made under my supervision at \dots\ department in partial fulfillment of the requirements 
  for the Bachelor Degree in \dots.
  
  \vspace{1cm}
  Supervisor Name \hfill Signature \hfill Date
\end{boxedlist}

% ------------------------- Examination Committee -------------------
\clearpage
\addcontentsline{toc}{chapter}{Examination Committee}
\begin{boxedlist}{Examination Committee}
  \noindent\textbf{Project Title:} \dotfill

  \vspace{0.8cm}
  \noindent\textbf{Supervisor}\\[0.3cm]
  \begin{tabular}{@{}p{0.6\linewidth}p{0.2\linewidth}p{0.15\linewidth}@{}}
    \textbf{Name} & \textbf{Position} & \textbf{Signature}\\
    \rule{0.9\linewidth}{0.4pt} & \rule{0.9\linewidth}{0.4pt} & \rule{0.9\linewidth}{0.4pt}\\
  \end{tabular}

  \vspace{0.8cm}
  \noindent\textbf{Examination Committee}\\[0.3cm]
  \begin{tabular}{@{}p{0.6\linewidth}p{0.2\linewidth}p{0.15\linewidth}@{}}
    \textbf{Name} & \textbf{Position} & \textbf{Signature}\\
    \rule{0.9\linewidth}{0.4pt} & \rule{0.9\linewidth}{0.4pt} & \rule{0.9\linewidth}{0.4pt}\\
    \rule{0.9\linewidth}{0.4pt} & \rule{0.9\linewidth}{0.4pt} & \rule{0.9\linewidth}{0.4pt}\\
    \rule{0.9\linewidth}{0.4pt} & \rule{0.9\linewidth}{0.4pt} & \rule{0.9\linewidth}{0.4pt}\\
  \end{tabular}

  \vspace{0.6cm}
  Department Head \dotfill
\end{boxedlist}
% ------------------------- جدول المحتويات والقوائم -----------------
% ===================== Front Matter Lists =====================
% نجعل الصفحات التمهيدية بالأرقام الرومانية:


%------------------- Boxed Title Helper -------------------
% دالة مساعدة لعنوان داخل إطار كما في الصورة
\newcommand{\boxedtitle}[1]{%
  \begin{center}
    \begin{mdframed}[linewidth=0.8pt, innertop=8pt, innerbottom=8pt, innerleft=14pt, innerright=14pt]
      \centering \Large\bfseries #1
    \end{mdframed}
  \end{center}
}

%------------------- Table of Contents -------------------
\clearpage
\addcontentsline{toc}{chapter}{Table of Contents} % إدراج بالفهارس (روماني)
\begin{boxedlist}{Table of Contents}
  % نخفي عنوان \tableofcontents الافتراضي حتى لا يتكرر
  \begingroup
    \renewcommand{\contentsname}{}%
    \tableofcontents
  \endgroup
\end{boxedlist}


%------------------- List of Figures ---------------------
\clearpage
\addcontentsline{toc}{chapter}{List of Figures}
\begin{boxedlist}{List of Figures}
  \begingroup
    \renewcommand{\listfigurename}{}%
    \listoffigures
  \endgroup
\end{boxedlist}

%------------------- List of Tables ----------------------
\clearpage
\addcontentsline{toc}{chapter}{List of Tables}
\begin{boxedlist}{List of Tables}
  \begingroup
    \renewcommand{\listtablename}{}%
    \listoftables
  \endgroup
\end{boxedlist}

\begin{comment}
%------------------- List of Abbreviations ---------------
\clearpage
\addcontentsline{toc}{chapter}{List of Abbreviations}
\begin{boxedlist}{List of Abbreviations}
  \begin{center}
    \begin{tabular}{@{}p{0.22\linewidth}p{0.70\linewidth}@{}}
      \toprule
      \textbf{Acronym} & \textbf{Definition} \\
      \midrule
      ARQ  & Automatic Repeat Request \\
      AWGN & Additive White Gaussian Noise \\
      BS   & Base Station \\
      CA   & Carrier Aggregation \\
      CoMP & Coordinated MultiPoint Transmission \\
      \bottomrule
    \end{tabular}
  \end{center}
\end{boxedlist}

%------------------- Mathematical Notation (Optional) ----
\clearpage
\addcontentsline{toc}{chapter}{Mathematical Notation (Optional)}
\begin{boxedlist}{Mathematical Notation (Optional)}
  \begin{center}
    \begin{tabular}{@{}p{0.28\linewidth}p{0.62\linewidth}@{}}
      \toprule
      \textbf{Parameter} & \textbf{Description} \\
      \midrule
      $B_{\!RB}$    & Resource block bandwidth. \\
      $E_c^{(0)}$   & Available energy in relay $i$ at the end of frame $i$. \\
      % أضف المزيد حسب حاجتك
      \bottomrule
    \end{tabular}
  \end{center}
\end{boxedlist}

\end{comment}
% ------------------------- بدء المتن (أرقام عربية) -----------------
\clearpage
\pagenumbering{arabic} % يبدأ Chapter 1 من صفحة 1 بالأرقام العربية


% ----------------------------- Page Style -----------------------------
\pagestyle{fancy}
\fancyhf{}
\lhead{Analytical Intelligence Project}
\rhead{Chapter One: Introduction}
\cfoot{\thepage}

% ------------------------- الفصل 1: Introduction -------------------
\chapter{Introduction}
% بإمكانك اتباع تنظيم فصل المقدمة كما ظهر في جدول المحتويات المقترح بالكتيّب:
\section{Overview}
With the significant expansion we are witnessing in recent years due to the use of digital systems and a wide range of online services, the collection and analysis of system logs has become critically important to ensure information security, detect potential threats, and develop effective countermeasures. Logs are a rich source of information about activities occurring within systems and networks, as they record everything that happens in the system. However, the challenge lies in analyzing the massive volume of data quickly and efficiently to enable the early detection of attacks or abnormal behaviors.

a lot of companies tend to use some of the famous systems such as Splunk\footnote{Splunk is a commercial platform for searching, monitoring, and analyzing machine-generated data via a web-style interface.}, Graylog\footnote{Graylog is an open-source log management tool for collecting, indexing, and analyzing both structured and unstructured data.}, and other Security Information and Event Management (SIEM) platforms.

these tools are often expensive and complex for many organizations with limited budgets.

This is where our project idea, "Analytical Intelligence" and we call it (AI), comes in — a simple yet intelligent tool for collecting and analyzing system logs, supported by artificial intelligence techniques to detect the most common anomalies and attacks, using flexible software technologies.
\section{Problem Statement}
Many small organizations lack systems for monitoring and analyzing logs to detect any problems they may encounter, especially on Ubuntu servers. This is due to the difficulty of managing these servers, often due to the high cost or technical complexity of the systems. This has led to numerous risks, including a limited ability to detect attacks at an early stage, difficulty tracking security incidents and analyzing their causes, and reliance on manual methods, which has slowed operations and increased the number of vulnerabilities that cause various problems.

Therefore, a software solution was needed that was easy to deploy, affordable, and supported intelligent log analysis.

\section{Project Objectives}
The primary goal of this project is to develop a simple and intelligent tool for collecting and analyzing Ubuntu server logs via a dashboard, using modern technologies and artificial intelligence to detect suspicious activity and generate reports. Sub-goals include designing an easy-to-use graphical interface for viewing and analyzing logs, supporting log collection from Ubuntu servers, implementing artificial intelligence algorithms for automatic anomaly detection, and providing alerts when suspicious activity occurs.


\section{Project Scope and Limitations}
\begin{itemize}[leftmargin=*]
   \item This tool is specifically designed for Ubuntu server environments and is not intended to replace full-scale business SIEM platforms.
   \item The tool focuses on collecting logs that may have the most common attacks.
   \item Accuracy depends on the quality and quantity of available log data that it collected.
   \item Machine learning algorithms may occasionally produce false positives.
   \item The dashboard displays results and reports in PDF and CSV formats.
\end{itemize}
\section{Project Methodology}
This project takes a systematic approach to designing, developing, and evaluating an intelligent, easy-to-use log analysis system for Ubuntu server environments. It aims to support security monitoring through intelligent anomaly detection in logs, leveraging locations that enhance alert accuracy, and artificial intelligence models that provide more accurate information.

The methodology consists of the following key phases:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Requirement Analysis:} Identifying the requirements and needs of small and medium-sized organizations regarding log management, especially on Ubuntu-based servers.
    
    \item \textbf{Log Collection Design:} Configure a secure and modern log collection pipeline with Filebeat to collect logs from Ubuntu Server environments.

    \item \textbf{Log Preprocessing:} Filter, parse, and normalize collected logs into a structured format suitable for machine learning and analysis.

    \item \textbf{Anomaly Detection Engine:} Implement machine learning models such as Isolation Forest to automatically detect anomalous behaviors or suspicious in logs.

    \item \textbf{Dashboard Interface:} Develop a simple and interactive web dashboard by using Flask to display real-time system status, alerts, and reports.

    \item \textbf{Testing and Evaluation:} Experiment with the system and test its anomaly detection performance using real logs from ubuntu server to improve response.
\end{enumerate}
\begin{comment}
    % إضافة الصورة
\begin{figure}[H] % الصورة ثابتة في نفس المكان  [H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/methodology.png} % ضع هنا مسار الصورة
    \caption{Project Methodology Diagram}
    \label{fig:methodology}
\end{figure}
\end{comment}


\section{Report Organization}
This report is divided into  chapters, each one will focus on a part of the project:

\begin{itemize}[leftmargin=*]
    \item \textbf{Chapter 1:} Introduces the overview, problem statement, objectives, methodology, and scope of the project.
    \item \textbf{Chapter 2:} Reviews related work, and tools relevant to log analysis and anomaly detection.
    \item \textbf{Chapter 3:} Describes the architecture, components of the system, and project models.
    \item \textbf{Chapter 4:} Explains the design of the system, how it works, and some project codes.
    \item \textbf{Chapter 5:} Details the implementation process and which tools are used.
    \item \textbf{Chapter 6:} Summarizes the results and discussions.
    \item \textbf{Chapter 7:} Conclusion and suggests improvements and potential future recommendations.
    \item \textbf{Chapter 8:} References of the project.
\end{itemize}

% ------------------------- الفصل 2: Background & LR ----------------

% ----------------------------- Page Style -----------------------------
\pagestyle{fancy}
\fancyhf{}
\lhead{Analytical Intelligence Project}
\rhead{Chapter Two: Background and Literature Review}
\cfoot{\thepage}

\chapter{Background and Literature Review}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/Background ch2.png}
    \caption{System Block Diagram}
    \label{fig:block}
\end{figure}

\section{Background}
\subsection{Cybersecurity}
Humanity today lives in an unprecedentedly interconnected era, where technology has become the backbone of daily life. It is no longer just an invention serving humanity; it is an indispensable infrastructure supporting several important sectors, such as communications, finance, corporate governance, and public services. However, this renaissance always comes with downsides and risks. Any flaw or vulnerability in digital systems can directly impact individuals, businesses, and even entire nations.

Therefore, cybersecurity has emerged as a fundamental response to modern stability, protecting data and digital assets from theft, tampering, and disruption. Societies unable to protect their infrastructure are at risk of collapse and social unrest. Today, cybersecurity is just as important as physical security for maintaining trust and enabling progress.

\subsection{System Security}
Cybersecurity is a comprehensive framework designed to protect organizations and individuals from various cyber threats. The increasing reliance on internet-connected devices has led to new challenges, including data leakage, hacking, and cyberattacks that attempt to exploit security vulnerabilities.

System security addresses these risks through several layers that contribute to mitigating vulnerabilities, such as network security, endpoint security, authentication and authorization mechanisms such as two-factor authentication or biometrics, security, and incident response. These measures ensure that systems remain robust against exploitation and unauthorized access.

\subsection{Servers}
Servers are the backbone of digital infrastructure, designed to provide high speed, resources, and shared services to multiple users simultaneously. Unlike personal computers, servers are optimized for stability, scalability, continuous operation, and high-volume operation. They host websites, databases, email systems, and cloud software.

Given their critical importance, servers are a prime target and frequent victim of cyberattacks. A single compromised server can allow attackers to access sensitive user data or control entire networks. There are different types of servers, such as:
\begin{itemize}[leftmargin=*]
   \item Windows Server
   \item Ubuntu Server
   \item RHEL Server, and a lot of different OS.
\end{itemize}

\subsection{Ubuntu Server}
Operating systems form the foundation of digital infrastructure, acting as an intermediary between devices and users. Any vulnerability in this layer could open the door to hackers.

Among the most widely used operating systems in enterprise environments are Ubuntu servers, known for their speed and high security. Servers host websites, databases, and cloud software, making them prime targets for cyberattacks. Therefore, server security includes:
\begin{itemize}[leftmargin=*]
   \item Regular fixes and updates.
   \item Strict access control, permission management, and privilege escalation.
   \item Firewall configuration and intrusion prevention.
   \item Continuous monitoring through Dashboard interface.
\end{itemize}

\subsection{Logs}
Logs are the living memory of any system, recording every event, transaction, and anomaly that occurs on the system. In Ubuntu Server environments, they include:
\begin{itemize}[leftmargin=*]
   \item Management logs.
   \item Security logs.
   \item System logs.
   \item Applecation logs.
   \item Network logs.
\end{itemize}

\subsection{Security Logs}
Security logs are a specialized category of log files that focus on events that impact system security. These logs include firewall activity, failed or repeated login attempts, privilege escalations, and any suspicious behavior that might indicate an attempted attack.

The main objectives of security logs are:
\begin{itemize}[leftmargin=*]
    \item \textbf{Threat Detection:} Identifying unauthorized access attempts and malicious activities.
    \item \textbf{Forensics:} It is considered a strong reference in the event of a crime.
    \item \textbf{Real-Time Monitoring:} Enabling administrators to receive alerts and respond to threats immediately.
\end{itemize}

\subsection{Anomaly Detection}
As organizations grow, the volume of stored logs increases, often reaching millions of events per day.

Manual review becomes impossible, leading to the adoption of advanced log management platforms such as Graylog and Splunk. These systems focus on log collection and issue real-time alerts for critical events.

The real challenge is distinguishing normal system behavior from malicious or suspicious patterns.

Anomaly detection addresses this by automatically identifying anomalies, such as:

\begin{itemize}[leftmargin=*]
    \item Rule-Based Detection.
    \item Rule-Based ML
    \item Heuristic Analysis
\end{itemize}

\subsection{Rule-Based Machine Learning}
One of the most popular methods is rule-based, but it sometimes produces many false positives or fails to detect new attack methods.

Machine learning (ML) provides a more adaptive approach, learning the system's normal behavior and identifying events that may be deviant.

ML techniques used for anomaly detection include:
\begin{itemize}[leftmargin=*]
    \item Supervised learning algorithms (like classification models).
    \item Unsupervised learning methods (like Isolation Forest, K-Means).
    \item Deep learning models (like LSTM networks for time-series logs).
\end{itemize}

\section{Literature Review}
\subsection{Introduction}
The literature review aims to provide an overview of studies related to our system and previous approaches in the field of log analysis, anomaly detection, and methods, focusing on their methodologies, strengths, weaknesses, and the tools used.
By analyzing this work, we can identify what has been achieved to date and the gaps that remain open for future research and development.

This review also discusses the similarities between the various literatures and compares them to our proposed system.

Finally, it highlights the research gap we seek to address through our project.

\subsection{Zero-Day Exploit Detection in Network Flows}
According to \cite{Toure2024}, a hybrid framework combining supervised classification,
unsupervised clustering, and online learning was proposed to detect zero-day attacks.

Using datasets such as IBM and NSL-KDD, the system achieved high accuracy in identifying zero-day vulnerabilities, while minimizing false positives.

This study emphasizes the need to integrate diverse learning approaches, but also highlights challenges related to computational complexity and feature engineering.

\subsection{Optimized Deep Isolation Forest (ODIF)}
Galka (2025) presents ODIF, an improved version of Deep Isolation Forest, which reduces computational costs and memory usage while maintaining anomaly detection accuracy.

By simplifying data representation and eliminating duplicate processing, ODIF facilitates isolation-based anomaly detection in large-scale or resource-constrained environments.

This work demonstrates how efficiency improvements can enhance the applicability of AI methods in cybersecurity, but they are weak when the tree depth is large. 

\subsection{Collaborative Intrusion Detection Systems (CIDS)}
Gómez et al. (2025) present a framework for collaborative intrusion detection in log data, emphasizing decentralized cooperation 
across organizations and systems.  
The study categorizes detection approaches into sequential, embedding, and graph-based methods, and highlights the role 
of collaboration in reducing false positives.  
Their open-source evaluation platform provides a structured benchmark for testing algorithms, underlining the need 
for reproducibility and standardized assessment in log anomaly detection research.  

\subsection{ADALog: Adaptive Unsupervised Anomaly Detection in Logs}
Zaremba et al. (2025) propose ADALog, a self-attention–based framework that leverages a pre-trained DistilBERT model 
to detect anomalies in unstructured logs without requiring labeled data or parsing.  
The model achieves competitive performance on benchmark datasets while improving interpretability through heatmap visualization.  
However, its reliance on high computational resources and batch processing indicates the need for further optimization.  

\subsection{LogBERT: Log Anomaly Detection via BERT}
Guo et al. (2021) introduce LogBERT, a framework that applies BERT to system logs using self-supervised learning.  
The model captures contextual dependencies through tasks such as masked log key prediction and clustering of normal behavior.  
Experiments on datasets like HDFS and BGL show that LogBERT outperforms traditional and deep learning baselines.  
While effective, it relies on log parsers and hyperparameter tuning, which may limit adaptability in dynamic environments.  

\subsection{LAnoBERT: Log Anomaly Detection without Log Parsers}
Lee, Kim, and Kang (2021) extend LogBERT by removing the dependency on log parsers, enabling direct analysis of raw log data.  
Using BERT’s masked language modeling, LAnoBERT improves flexibility and robustness across different log formats, 
achieving competitive performance compared to parser-based models.  
Its strengths lie in simplicity and adaptability, though it remains computationally demanding and less interpretable 
than other approaches.  

\subsection{Temporal Logical Attention Network (TLAN)}
Liu et al. (2024) propose TLAN, a model that captures temporal and logical relationships in log sequences using multi-scale 
feature extraction and attention mechanisms.  
Tested on distributed system logs, it improves detection of multi-step attacks such as SSH brute force followed 
by privilege escalation.  
Despite higher computational requirements, TLAN demonstrates the importance of modeling sequential dependencies 
for advanced log anomaly detection.  

\subsection{Semi-Supervised Framework for ALICE O$^{2}$}
Krupa et al. (2025) develop a semi-supervised framework for real-time anomaly detection in CERN’s ALICE O$^{2}$ logs.  
By combining limited labeled anomalies with abundant normal data, the system balances supervised and unsupervised learning, 
improving detection accuracy and adaptability.  
This approach shows promise for large-scale infrastructures, though scalability and generalization remain key challenges 
for broader adoption.  

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/table-of-researches.png}
    \caption{Literature Review}
    \label{fig:LiteratureReview}
\end{figure}

\section{Tools}
In recent years, researchers have developed specialized tools for log management and security analysis. These tools range from free to paid, each designed to meet organizations' needs in a way that best suits them. Most tools are designed for environments that handle massive amounts of data. This diversity is driven by the growing demand for flexible, scalable, and reliable cybersecurity solutions. Here, we review the most popular and widely used tools.
\subsection{Graylog}
Graylog is an open-source log management platform that allows centralized log collection, storage, and analysis. It provides dashboards, search functionality, and real-time alerts, making it a great option for small and medium-sized organizations.

\subsection{Splunk}
Splunk is a widely used enterprise-grade tool for real-time log analysis and SIEM. It excels at processing large datasets and offers advanced search and visualization features. However, it comes with significant licensing costs.

\subsection{Elastic Stack (ELK)}
The ELK stack (Elasticsearch, Logstash, Kibana) is a flexible, open-source solution for log ingestion, indexing, and visualization. It's highly customizable, but requires technical expertise for deployment and maintenance.

\subsection{LogBERT}
LogBERT is a research-oriented framework that uses BERT-based models to detect anomalies in logs. It doesn't require labeled data and provides high accuracy, making it effective for server and operating system environments.

\subsection{ADALog}
ADALog uses DistilBERT for unsupervised anomaly detection in unstructured logs. It's accurate and interpretable, but it works in batch mode and needs high-performance hardware.

\subsection{Sumo Logic}
Sumo Logic is a cloud-native log analytics platform that integrates machine learning for security monitoring and performance insights. It offers scalability and is ideal for cloud-based systems.

\subsection{Datadog Log Management}
Datadog integrates log collection with infrastructure and performance metrics. It's effective for distributed systems and microservices, offering unified observability.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/table-of-tools.png}
    \caption{Literature Review}
    \label{fig:LiteratureReview}
\end{figure}

\section{Gab Research}

Despite advances presented in previous studies, ranging from hybrid zero-day vulnerability detection frameworks to transformer-based models such as LogBERT, LAnoBERT, and ADALog, several challenges remain:

\begin{itemize}
    \item \textbf{High computing requirements:} Most existing approaches, particularly those based on transformers, rely on high computing resources, limiting their deployment to small and medium-sized enterprises with limited resources.
    
    \item \textbf{Reliance on standard datasets only:} Despite achieving strong results on datasets such as HDFS and BGL, these models are rarely tested on live Ubuntu logs, weakening their reliability in practical use.
    
    \item \textbf{Balancing scalability and ease of use:} Deep learning models achieve high accuracy, but they are often complex to configure and operate, making them unsuitable for non-expert administrators.
    
    \item \textbf{Lack of standard frameworks and standardized evaluation tools:} The lack of reproducible testing environments or standardized evaluation metrics makes it difficult to compare models and generalize results across different environments.
    
    \item \textbf{Poor explainability:} Some models, such as ADALog, offer good accuracy but lack clear mechanisms for explaining their decisions, which reduces security administrators' confidence in the results.
\end{itemize}

These vulnerabilities highlight the need for a simple, flexible, and easy-to-interpret anomaly detection framework that balances efficiency and ease of use and provides immediate alerts. Our project addresses these needs by developing an AI-powered log analysis platform, specifically designed for Ubuntu servers, with the goal of reducing costs, simplifying deployment, and improving anomaly detection accuracy without requiring advanced hardware or expertise.

% =========================== الفصل 3: Requirements ======================
\pagestyle{fancy}
\fancyhf{}
\lhead{Analytical Intelligence Project}
\rhead{Chapter Three: Requirements Analysis and Modeling}
\cfoot{\thepage}

\chapter{Requirements Analysis and Modeling}

\section{Introduction}
In this chapter, we outline the key requirements for an analytical intelligence (AI) tool, which combines speed and simplicity to collect, process, and analyze security logs from an Ubuntu server.

Unlike heavy-duty SIEM systems, this system focuses on clarity and low costs, making it suitable for small and medium-sized businesses. This chapter presents the foundations of design and modeling by defining the architecture, feasibility, methodology, and requirements. At the end of this chapter, we present models and diagrams (block diagram, use case, DFD, and database schema) to provide a clearer picture of the system and understand the requirements.

\section{System Description}
Analytical Intelligence (AI) is based on a multi-layered architecture focused on speed and ease of use. Specifically used for Ubuntu servers, each layer has a stage that explains a portion of the system's log lifecycle—from log collection, through filtering and cleaning, to detection/analysis, and finally, reporting. The layers are interconnected via flowcharts that illustrate the system's operation, allowing for continuous updates and work.

\section{Feasibility Study}
The feasibility of the system system will be evaluated through feasibility testing (technical, legal, operational and economic).

\subsection{Technical Feasibility}
The system relies on modern, widely adopted open source technologies, such as Ubuntu Server, Docker, FastAPI, PostgreSQL, and NFStream. These technologies and tools have proven their technical proficiency and robustness, making them reliable components that ensure stability, scalability, and reliability even under challenging conditions.

The system uses Python for analysis, enrichment, and anomaly detection, making it flexible, given that it is one of the most widely used and powerful programming languages in the field of artificial intelligence. It also includes comprehensive libraries dedicated to data processing, machine learning, and cybersecurity.

By leveraging this technology stack, the system avoids the costs and limitations of proprietary solutions, while maintaining the adaptability needed to integrate additional modules in the future. Its robust architecture allows organizations to start with an easy setup, running on a single server, and scale to multiple servers as log volumes or analysis requirements increase.

\subsection{Legal Feasibility}
Legal feasibility is a critical factor for any system intended for distribution to businesses and others, as records often contain sensitive details such as IP addresses, usernames, and timelines of user activity, all of which may fall under privacy regulations.

To mitigate risks, the system is designed with strict adherence to internationally recognized frameworks such as the General Data Protection Regulation (GDPR). Encryption is applied to data in transit and storage, while role-based access control ensures that only authorized users have access to sensitive information, with user control controlled by the administrator.

The system relies exclusively on open source components subject to permissive licenses such as the MIT License, which legally permits the code to be used for any purpose, whether commercial or non-commercial. This ensures that organizations adopting this system face minimal legal risk while maintaining compliance with global standards such as ISO/IEC 27001.

\subsection{Operational Feasibility}
Operational feasibility is essential for a system, determining its effectiveness in real-world environments.

The platform is designed to be easy and practical, capable of operating efficiently even with medium-performance hardware. Its dashboards and analysis capabilities are simple and intuitive, enabling administrators and analysts to quickly interpret security events without the need for advanced security expertise.

This ease of use significantly reduces training and time requirements, especially for academic institutions and small and medium-sized businesses that may not have dedicated security teams.

We also offer real-time alerts, ensuring that incidents such as brute-force login attempts, privilege escalation, or suspicious IP addresses are detected and reported immediately to the appropriate personnel.

\subsection{Economic Feasibility}
Most importantly, it's economical. The system is designed to reduce overall costs while maintaining the option to expand when needed.

This solution relies on open source components with no licensing fees and can operate effectively on a single mid-range server in small or academic environments. Therefore, costs are concentrated on the one-time hardware acquisition and limited ongoing operations (electricity, storage, and simple administration time).

It's also supported by artificial intelligence, which reduces attack detection time and aids in rapid analysis of various logs.

\begin{table}[H]
\centering
\caption{Economic Feasibility — Cost Table (Lean)}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Item} & \textbf{Type} & \textbf{Frequency} & \textbf{Est. Cost (USD)} \\
\hline
Ubuntu server (8 vCPU, 8 GB RAM) & CAPEX & One-time & --- \\
SSD storage (512 GB) & CAPEX & One-time & 30 \\
Electricity & OPEX & Monthly & 10 \\
Internet & OPEX & Monthly & 14 \\
Team & OPEX & Monthly & 500 \\
\hline
\end{tabular}
\end{table}

\section{Methodology}
The system follows the Agile methodology. This approach allows the system to be implemented systematically while maintaining sufficient flexibility to accommodate improvements, feedback, and upcoming security challenges.

The process begins with the planning phase, where the overall project goals are defined and the key challenges of analyzing Ubuntu security logs are identified.

Following the planning phase, the requirements gathering phase is implemented, focusing on an in-depth analysis of security logs and the types of attacks they reveal. This phase leads to a comprehensive understanding of the logs and contributes to designing an architecture capable of handling them efficiently.

Then, the system design phase begins, producing architecture diagrams, workflows, and system models that formalize the layered architecture of AI. The focus is on developing each layer—collection, processing, detection, and presentation—independently without disrupting the entire system.

\subsection{Data Collection Layer}
The data collection layer is the core of the system, and its primary purpose is to collect real-time logs and network flows from the Ubuntu server. These are collected from multiple critical sources. Auth logs (/var/log/auth.log) capture failed login attempts, abusive SSH activity, and sudo usage, providing essential insights into authentication security. Network flows are captured using NFStream, which extracts bidirectional flow statistics from live network interfaces, including packet counts, byte counts, and inter-arrival time metrics.

The system uses lightweight Python-based collector agents that tail log files and capture network flows in real-time. These agents transmit data to the centralized backend via HTTP API calls with API key authentication. The system incorporates secure transport protocols to protect data in transit, ensuring confidentiality and integrity.

\subsection{The Processing Layer}
When logs and flows are collected, they move to the processing layer, where the raw data is transformed into a format suitable for structured analysis. The processing layer is responsible for normalization, enrichment, and filtering, enabling consistent and accurate analysis across different data sources. Enrichment adds contextual information to each event, such as severity levels based on predefined criteria. Filtering mechanisms then remove duplicate data and irrelevant noise, retaining only useful data for further examination. This layer leverages a FastAPI backend for processing and transformation, along with custom Python modules that enable advanced enrichment and domain-specific filtering rules. Raw events are stored as JSONB in PostgreSQL for flexible querying. This results in organized and consistent data.

\subsection{Detection and Analysis Layer}
At the detection and analysis layer, structured data is transformed into actionable security intelligence. The system employs \textbf{two specialized machine learning models} running server-side:

\textbf{1. SSH LSTM Model (ssh\_lstm):} This model analyzes authentication events from /var/log/auth.log. It operates in dual mode:
\begin{itemize}
    \item \textbf{Threshold-Based Detection:} Tracks failed login attempts per source IP. When the count exceeds \texttt{SSH\_BRUTEFORCE\_THRESHOLD} (default: 5) within \texttt{SSH\_BRUTEFORCE\_WINDOW\_SECONDS} (default: 300), a brute force alert is triggered.
    \item \textbf{Sequence-Based Detection:} An LSTM neural network analyzes token sequences (FAILED\_PASSWORD, INVALID\_USER, ACCEPTED\_PASSWORD, etc.) to detect anomalous authentication patterns that may indicate sophisticated attacks.
\end{itemize}

\textbf{2. Network RF Model (network\_rf):} A Random Forest classifier trained on CIC-IDS2017 dataset features. It classifies network flows into attack categories:
\begin{itemize}
    \item DoS (Denial of Service)
    \item DDoS (Distributed Denial of Service)
    \item Port Scanning
    \item Brute Force (network-level)
    \item Web Attacks
    \item Bots
    \item Normal Traffic
\end{itemize}

The network model uses 52 flow-level features including packet lengths, inter-arrival times, and TCP flags. Post-prediction filters include label allowlisting, confidence thresholds (\texttt{NETWORK\_ML\_THRESHOLD}), and deduplication to prevent alert storms.


\subsection{Presentation Layer}
The presentation layer sits at the top of the system architecture and is designed to provide administrators and security analysts with a clear, intuitive, and actionable interface for system activity. A custom web dashboard built with Jinja2 templates and FastAPI is used to create interactive dashboards that transform log and detection data into clear insights. Dashboards present data through visual elements such as severity badges, detection tables, and device statistics pages that track the evolution of events over time.

The presentation layer also supports the creation of exportable reports in formats such as PDF and CSV. Combining ease of use with powerful analytical capabilities, this layer ensures that even users with limited technical expertise can easily monitor the system through an interface that provides the essential monitoring requirements.

\section{Dataset Description}
For training and evaluating our network intrusion detection model, we used the CIC-IDS2017 dataset. This dataset was developed by the Canadian Institute for Cybersecurity (CIC) at the University of New Brunswick. It contains labeled network traffic data captured over five days, including both normal (benign) traffic and various attack types.

The dataset includes network flow features extracted from packet captures, covering attacks such as DoS, DDoS, Brute Force, Port Scanning, Web Attacks, and Botnet traffic. The diversity of attack types makes it highly suitable for training multi-class classification models for network intrusion detection.

We selected this dataset for several reasons:

\begin{itemize}
	\item \textbf{Relevance:} It contains network flow data with statistical features similar to those extracted by NFStream in our live collection pipeline.
	
	\item \textbf{Attack Coverage:} It includes multiple attack categories (DoS, DDoS, Brute Force, Port Scanning, Web Attacks, Bots) that align with our detection goals.
	
	\item \textbf{Class Labels:} The dataset provides clear attack labels, enabling supervised training of our Random Forest classifier.
	
	\item \textbf{Benchmarking:} CIC-IDS2017 is widely used in academic research, enabling comparison with published intrusion detection systems.
	
	\item \textbf{Feature Compatibility:} The 78 network flow features can be mapped to NFStream output, enabling model deployment on live traffic.
\end{itemize}

By training on the CIC-IDS2017 dataset, we ensure that the developed system can classify network attacks with high accuracy while maintaining practical applicability to real-world network flows.

\section{End-to-End Workflow}
The project will be implemented in a structured and carefully planned manner to ensure quality and reliability of the system. 

First, the critical Ubuntu security logs and network flows are enabled, validated, and synchronized to guarantee accurate event recording. Auth logs are collected by the auth\_collector agent which tails /var/log/auth.log, while network flows are captured by the flow\_collector agent using NFStream. These events are then parsed, normalized into JSON format, and enriched with contextual information such as severity levels to facilitate search, filtering, and correlation. The processed events are stored in PostgreSQL with JSONB payload fields to maintain fast and efficient queries.

After a short preparation period, the system establishes detection capabilities through two complementary approaches: (1) threshold-based mechanisms for well-defined patterns, such as repeated failed SSH login attempts within a configurable time window (SSH Brute Force detection using the SSH LSTM detector), and (2) machine learning classification using a Random Forest model trained on network flow features to identify DoS, DDoS, Port Scanning, and Network Brute Force attacks. Each flagged event is assigned a confidence score and severity level to support analyst understanding and prioritization.

Finally, all results are reviewed through an interactive web dashboard that supports filtering, search, and forensic auditing. Reports summarizing security events are exported in CSV and XLSX formats. The system also supports real-time Telegram alerts for high-severity detections. Periodically, models can be retrained to reflect new attack patterns, keeping the system effective over time.

\section{Functional Requirements}
\subsection{Scope \& Collection}
The system's functional requirements define the core capabilities the platform must provide to achieve its goals as an intelligent security monitoring tool. The system must collect two types of data in real time: (1) authentication logs from /var/log/auth.log capturing SSH login attempts and sudo usage, and (2) network flows from live packet capture using NFStream. The system standardizes these data sources into JSON format for consistent analysis. This requirement ensures compatibility across diverse data sources and enables both auth-based and network-based threat detection.

\subsection{Detection \& Analysis}
Once data is collected and normalized, the system provides robust analysis through two detection mechanisms. For authentication logs, the SSH LSTM detector tracks failed login attempts per source IP and triggers brute force alerts when the count exceeds a configurable threshold within a time window. For network flows, a Random Forest classifier trained on CIC-IDS2017 features predicts attack types (DoS, DDoS, Port Scanning, Brute Force) with associated confidence scores. The network detector includes post-prediction filters: label allowlisting, confidence thresholds, and deduplication to prevent alert storms. Each detection is assigned a severity level (low, medium, high, critical) based on attack type and confidence. Together, these dual modes of detection provide comprehensive coverage of both auth-based and network-based threats.

\subsection{Real-Time Alerting}
Another essential functional requirement is real-time alerting. Detection is meaningless without rapid communication of results. The system delivers timely notifications through two channels: (1) a web dashboard showing recent detections with severity badges and filtering, and (2) Telegram bot integration that pushes alerts for high-severity detections directly to security analysts' devices. The Telegram notifier includes rate limiting and severity thresholds to prevent alert fatigue.

\subsection{Visualization, Search, Forensics, and Reporting}
In addition to alerts, AI should provide simple visualization capabilities. Through user-friendly dashboards built using FastAPI and Jinja2 templates, administrators should be able to visualize security events in graphical form. These dashboards transform complex log data into clear, actionable insights. Additionally, the system should support powerful search and query features that enable users to filter and investigate logs based on criteria such as date range, IP address, severity level, or event type. This functionality is critical for forensic investigations and reconstructing the sequence of events surrounding an incident. Finally, AI should support automated reporting by generating summaries in multiple formats, including CSV and XLSX, for compliance audits and corporate documentation.

\section{Non-Functional Requirements}
\subsection{Performance \& Scale}
The system must meet a set of non-functional requirements to ensure its reliability, ease of use, and long-term sustainability. Performance is a key attribute, as the system must be capable of processing large volumes of security logs at scale. The AI is expected to handle at least several logs per second on mid-range devices, while maintaining minimal latency between the occurrence of an event and its visualization on the dashboard. This performance objective ensures that the system can support both small-scale academic deployments and larger enterprise environments as it scales. Scalability is closely linked to performance. The system must be designed to support expansion, both through increased hardware and the addition of multiple devices. This requirement ensures that the AI ​​remains flexible and adaptable to organizations of varying sizes and evolving needs.

\subsection{Security}
Security itself constitutes a non-functional requirement, since the system is entrusted with sensitive log data. To this end, AI must guarantee confidentiality and integrity by employing modern encryption protocols such as TLS 1.3 for communication, and by encrypting stored log data at rest. Moreover, access must be restricted through role-based access control (RBAC), ensuring that administrators, analysts, and general users are granted permissions appropriate to their roles. System activity must also be logged internally, providing accountability and an audit trail of configuration changes, login attempts, and data access events.

\subsection{Usability \& Reliability}
Usability is another critical requirement. AI is intended to be accessible not only to experienced cybersecurity professionals but also to junior administrators and researchers. The dashboards and interfaces must therefore be designed for clarity and simplicity, supported by comprehensive documentation and training materials to minimize the learning curve. The system must also embody reliability and maintainability. Reliability requires high availability, with a target uptime of at least 99 percent, supported by redundancy and failover mechanisms. Backup and recovery procedures must be in place to ensure that logs are not lost due to hardware failures or software crashes. Maintainability is equally important, as the landscape of security threats and regulatory requirements evolves constantly. The modular design of AI facilitates updates and enhancements to individual components—such as anomaly detection algorithms or data parsers—without disrupting the entire system. Regular updates and detailed technical documentation must be provided to ensure smooth operation and continuous improvement.

\subsection{Compliance}
Finally, compliance with international and local standards such as ISO/IEC 27001 is a fundamental non-functional requirement. By aligning with these standards AI not only provides technical robustness but also supports organizations in meeting audit and certification requirements. Collectivel, these non-functional requirements ensure that AI is secure, efficient, user-friendly, reliable, and adaptable for long-term deployment in diverse environments.

\section{System Modeling}
This section presents the system models that formalize the architecture and design of AI.

\subsection{Block Diagram}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/block-diagram.png}
    \caption{System Block Diagram}
    \label{fig:block}
\end{figure}

\subsection{Use Case Model}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/Use-Case-Diagram.png}
    \caption{Use Case Diagram}
    \label{fig:usecase}
\end{figure}

\subsection{DFD – Data Flow Diagrams}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/DFD-0.png}
    \caption{Data Flow Diagram}
    \label{fig:dfd}
\end{figure}

\section{Database Schema}
\label{sec:db-schema}
The system uses PostgreSQL as the relational database. PostgreSQL provides ACID compliance, robust querying capabilities, and supports JSONB for flexible storage of raw event payloads. The database schema consists of three main tables:

\subsection{Devices Table}
Stores registered sensor devices that send data to the system.

\begin{lstlisting}[style=python,caption=Devices Table Schema]
CREATE TABLE devices (
    device_id TEXT PRIMARY KEY,
    hostname TEXT,
    ip TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    last_seen TIMESTAMPTZ DEFAULT NOW(),
    os TEXT,
    role TEXT,
    tags TEXT
);
\end{lstlisting}

\subsection{Raw Events Table}
Stores all incoming events as-is with JSONB payloads for flexible querying.

\begin{lstlisting}[style=python,caption=Raw Events Table Schema]
CREATE TABLE raw_events (
    id BIGSERIAL PRIMARY KEY,
    ts TIMESTAMPTZ NOT NULL,
    device_id TEXT REFERENCES devices(device_id),
    event_type TEXT NOT NULL,  -- 'auth' or 'flow'
    payload JSONB NOT NULL
);
\end{lstlisting}

\subsection{Detections Table}
Stores detection records from both SSH LSTM and Network RF models.

\begin{lstlisting}[style=python,caption=Detections Table Schema]
CREATE TABLE detections (
    id BIGSERIAL PRIMARY KEY,
    ts TIMESTAMPTZ NOT NULL,
    device_id TEXT REFERENCES devices(device_id),
    raw_event_id BIGINT REFERENCES raw_events(id),
    model_name TEXT NOT NULL,  -- 'ssh_lstm' or 'network_rf'
    label TEXT NOT NULL,        -- attack type
    score DOUBLE PRECISION NOT NULL,
    severity TEXT NOT NULL,     -- LOW, MEDIUM, HIGH, CRITICAL
    details JSONB NOT NULL,
    occurrences BIGINT DEFAULT 1,
    src_ip TEXT,
    dst_ip TEXT,
    dst_port INT
);
\end{lstlisting}

\section{API Endpoints Summary}
\label{sec:api-summary}
Table~\ref{tab:api-endpoints} summarizes the key API endpoints exposed by the backend.

\begin{table}[H]
\centering
\caption{API Endpoints Summary}
\label{tab:api-endpoints}
\begin{tabular}{|l|l|l|p{5cm}|}
\hline
\textbf{Endpoint} & \textbf{Method} & \textbf{Auth} & \textbf{Purpose} \\
\hline
/api/v1/ingest/auth & POST & API Key & Ingest auth.log events (SSH model) \\
/api/v1/ingest/flow & POST & API Key & Ingest network flows (RF model) \\
/api/v1/health & GET & None & Health check \\
/api/v1/stats & GET & None & Dashboard statistics \\
/api/v1/recent-detections & GET & None & Recent detections JSON \\
/api/v1/devices & GET & None & Device list JSON \\
/api/v1/reports/export & GET & None & Export CSV/XLSX \\
\hline
\end{tabular}
\end{table}

\textbf{Ingestion Payload Schemas:}
\begin{itemize}
    \item \textbf{Auth Payload:} \texttt{device\_id}, \texttt{hostname}, \texttt{device\_ip}, \texttt{timestamp}, \texttt{line} (raw auth.log line)
    \item \textbf{Flow Payload:} \texttt{device\_id}, \texttt{hostname}, \texttt{device\_ip}, \texttt{flow} (NFStream flow statistics with 52+ features)
\end{itemize}



    % ------------------------- الفصل 4: Design -------------------------
\pagestyle{fancy}
\fancyhf{}
\lhead{Analytical Intelligence Project}
\rhead{Chapter Four: Project Design}
\cfoot{\thepage}

\chapter{Project Design}
\label{ch:design}

\section{Introduction}
This chapter presents the detailed design of the Analytical-Intelligence system, translating the requirements from Chapter~3 into concrete architectural decisions, component structures, and implementation patterns. The design emphasizes modularity, containerized deployment, and dual-model detection covering both authentication (SSH) and network traffic analysis.

\section{Distributed System Architecture}
\label{sec:distributed-arch}

\subsection{Unified Sensors and Intelligent Agents}
The system employs a two-stack architecture separating data collection from analysis:

\subsubsection{Network Level Monitoring (Flow Collector)}
The flow collector agent captures live network traffic using NFStream:
\begin{itemize}
    \item Runs in \texttt{network\_mode: host} for raw packet access
    \item Extracts bidirectional flow statistics (packets, bytes, IAT)
    \item Configurable idle/active timeouts for flow termination
    \item Maps flows to the \textbf{Network RF Model} pipeline
\end{itemize}

\subsubsection{Operating System Level Monitoring (Auth Collector)}
The auth collector agent monitors authentication events:
\begin{itemize}
    \item Tails \texttt{/var/log/auth.log} using file inode tracking
    \item Filters SSH/PAM-related events
    \item Implements automatic reconnection on failures
    \item Maps events to the \textbf{SSH LSTM Model} pipeline
\end{itemize}

\begin{table}[H]
\centering
\caption{Sensor to Model Mapping}
\label{tab:sensor-model}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Sensor} & \textbf{Data Source} & \textbf{Detection Model} \\
\hline
auth\_collector & /var/log/auth.log & SSH LSTM Model \\
flow\_collector & Network interface (NFStream) & Network RF Model \\
\hline
\end{tabular}
\end{table}

\subsection{Central AI Server (AI Central Brain)}
The central server hosts the FastAPI backend with the following components:

\begin{itemize}
    \item \textbf{Model Loader:} Loads both SSH LSTM and Network RF models at startup
    \item \textbf{Ingestion Router:} Handles \texttt{/api/v1/ingest/auth} and \texttt{/api/v1/ingest/flow}
    \item \textbf{Detection Engine:} Runs both models server-side with post-prediction filters
    \item \textbf{Database Layer:} Async PostgreSQL via SQLAlchemy
    \item \textbf{UI Router:} Serves Jinja2-templated dashboard pages
    \item \textbf{Notification Bus:} Async queue for Telegram alerts
\end{itemize}

\subsection{Communication Protocol}
All sensor-to-server communication uses HTTP POST with API key authentication:

\begin{itemize}
    \item \textbf{Header:} \texttt{X-API-Key: <INGEST\_API\_KEY>}
    \item \textbf{Content-Type:} \texttt{application/json}
    \item \textbf{Retry Policy:} Exponential backoff on connection failures
    \item \textbf{Batching:} Configurable batch size for flow collector
\end{itemize}

\textbf{Auth Payload Schema:}
\begin{lstlisting}[style=python,caption=Auth Event Payload]
{
    "device_id": "sensor-ubuntu-01",
    "hostname": "webserver-01",
    "device_ip": "192.168.1.100",
    "timestamp": "2025-01-20T10:30:00Z",
    "line": "Jan 20 10:30:00 sshd[1234]: Failed password..."
}
\end{lstlisting}

\textbf{Flow Payload Schema:}
\begin{lstlisting}[style=python,caption=Flow Event Payload]
{
    "device_id": "sensor-ubuntu-01",
    "hostname": "webserver-01", 
    "device_ip": "192.168.1.100",
    "flow": {
        "src_ip": "10.0.0.5",
        "dst_ip": "192.168.1.100",
        "dst_port": 80,
        "protocol": 6,
        "bidirectional_packets": 150,
        "bidirectional_bytes": 12000,
        ...  // 52 features total
    }
}
\end{lstlisting}

\section{Data Processing and Storage Design}
\label{sec:data-processing}

\subsection{PostgreSQL Database Design}
The database consists of three tables with foreign key relationships:

\begin{itemize}
    \item \textbf{devices:} Sensor registry with device\_id, hostname, IP, last\_seen
    \item \textbf{raw\_events:} All events stored with JSONB payload, event\_type (auth/flow)
    \item \textbf{detections:} Alert records with model\_name, label, score, severity, details
\end{itemize}

\subsection{Feature Engineering}

\subsubsection{SSH Model Features}
The SSH LSTM model uses token sequences derived from auth.log parsing:

\begin{table}[H]
\centering
\caption{SSH Token Vocabulary}
\label{tab:ssh-tokens}
\begin{tabular}{|l|l|}
\hline
\textbf{Token} & \textbf{Pattern} \\
\hline
FAILED\_PASSWORD & Failed password for \\
INVALID\_USER & Invalid user / Failed password for invalid user \\
ACCEPTED\_PASSWORD & Accepted password for \\
ACCEPTED\_PUBLICKEY & Accepted publickey for \\
PAM\_AUTH\_FAILURE & pam\_unix.*authentication failure \\
DISCONNECT & Disconnected from \\
CONNECTION\_CLOSED & Connection closed by \\
\hline
\end{tabular}
\end{table}

\subsubsection{Network RF Features}
The Network RF model uses 52 flow-level features mapped from NFStream output:

\begin{itemize}
    \item \textbf{Packet Statistics:} Total packets, lengths (min/max/mean/std)
    \item \textbf{Byte Statistics:} Total bytes, bytes/second
    \item \textbf{Inter-Arrival Time:} Flow IAT, Forward IAT, Backward IAT
    \item \textbf{TCP Flags:} FIN, PSH, ACK flag counts
    \item \textbf{Window Sizes:} Initial window bytes (forward/backward)
\end{itemize}

\section{Algorithm Design}
\label{sec:algorithm-design}

\subsection{Specialized Models (Server-Side)}

\subsubsection{SSH LSTM Model Design}
The SSH model operates in dual detection mode:

\textbf{1. Threshold-Based Detection:}
\begin{itemize}
    \item Tracks failed login attempts per source IP using \texttt{SSHEventTracker}
    \item Maintains rolling 1-hour window of failed attempts
    \item Triggers when count $\geq$ \texttt{SSH\_BRUTEFORCE\_THRESHOLD} (default: 5) within \texttt{SSH\_BRUTEFORCE\_WINDOW\_SECONDS} (default: 300)
    \item Labels: \texttt{SSH\_BRUTE\_FORCE}
\end{itemize}

\textbf{2. Sequence-Based Detection (LSTM):}
\begin{itemize}
    \item Analyzes token sequences using LSTM neural network
    \item Window size: configurable (default: 10 tokens)
    \item Predicts anomaly score (0.0 to 1.0)
    \item Triggers when score $\geq$ model threshold
    \item Labels: \texttt{SSH\_ANOMALY}
\end{itemize}

\subsubsection{Network RF Model Design}
The Network RF classifier uses a Random Forest trained on CIC-IDS2017:

\textbf{Feature Mapping Pipeline:}
\begin{enumerate}
    \item Receive NFStream flow data
    \item Map to 52 CIC-IDS2017 features using \texttt{network\_feature\_mapper.py}
    \item Apply preprocessing (scaling, NaN handling)
    \item Predict class and confidence using RF model
\end{enumerate}

\textbf{Post-Prediction Filters:}
\begin{itemize}
    \item \textbf{Label Allowlist:} Only DoS, DDoS, Port Scanning, Brute Force create detections
    \item \textbf{Confidence Threshold:} \texttt{NETWORK\_ML\_THRESHOLD} (default: 0.60)
    \item \textbf{Deduplication:} Same (src\_ip, dst\_ip, dst\_port, label) within window
    \item \textbf{Cooldown:} Per-source IP cooldown to prevent alert storms
\end{itemize}

\subsection{Decision Gating and Alert Quality Controls}
The system implements multiple gating layers to ensure alert quality:

\begin{table}[H]
\centering
\caption{Alert Quality Controls}
\label{tab:alert-controls}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Control} & \textbf{Configuration} & \textbf{Purpose} \\
\hline
Confidence Threshold & NETWORK\_ML\_THRESHOLD & Minimum ML score \\
Label Allowlist & NETWORK\_LABEL\_ALLOWLIST & Filter attack types \\
Deduplication Window & ML\_DEDUP\_WINDOW\_SECONDS & Aggregate duplicates \\
Source Cooldown & ML\_COOLDOWN\_SECONDS\_PER\_SRC & Limit per-IP alerts \\
SSH Threshold & SSH\_BRUTEFORCE\_THRESHOLD & Failed login count \\
SSH Window & SSH\_BRUTEFORCE\_WINDOW\_SECONDS & Time window \\
\hline
\end{tabular}
\end{table}

\subsection{Severity Assignment}
Severity levels are assigned based on attack type and confidence:

\begin{itemize}
    \item \textbf{CRITICAL:} DDoS with high confidence
    \item \textbf{HIGH:} DoS, SSH Brute Force with high failed count
    \item \textbf{MEDIUM:} Port Scanning, moderate confidence attacks
    \item \textbf{LOW:} Low confidence detections, SSH Anomaly
\end{itemize}

\section{Dashboard User Interface Design}
\label{sec:ui-design}
The web dashboard provides visibility into both SSH and Network RF detections:

\begin{table}[H]
\centering
\caption{Dashboard Pages and Endpoints}
\label{tab:ui-pages}
\begin{tabular}{|l|l|p{5cm}|}
\hline
\textbf{Page} & \textbf{Route} & \textbf{Content} \\
\hline
Dashboard & / & Statistics cards, recent detections \\
Alerts & /alerts & Filterable detection list (SSH + RF) \\
Auth Events & /events/auth & Raw auth.log entries \\
Flow Events & /events/flows & Raw network flow records \\
Devices & /devices & Sensor inventory with alert counts \\
Models & /models & SSH + RF model status/config \\
Reports & /reports & Export interface (CSV/XLSX) \\
\hline
\end{tabular}
\end{table}

\section{Summary}
This chapter presented the detailed design of Analytical-Intelligence, covering the two-stack distributed architecture with sensor-to-model mapping, the central AI server with dual model loading (SSH LSTM + Network RF), communication protocols, database design, algorithm design for both detection models, decision gating controls, and dashboard UI structure. The next chapter describes the implementation details.

% ------------------------- الفصل 5: Implementation & Test ----------
\pagestyle{fancy}
\fancyhf{}
\lhead{Analytical Intelligence Project}
\rhead{Chapter Five: Implementation and Test}
\cfoot{\thepage}

\chapter{Implementation and Test}
\label{ch:implementation}

\section{Chapter Introduction}
This chapter documents the implementation of the Analytical-Intelligence system, describing the development environment, software stack, implementation details for both sensor agents and the central AI server with its dual detection models (SSH LSTM and Network RF), and the testing strategy.

\section{Development Environment and Software Stack}
\label{sec:dev-env}

\subsection{Software Stack}
Table~\ref{tab:software-stack} lists the key technologies used in the implementation.

\begin{table}[H]
\centering
\caption{Software Stack}
\label{tab:software-stack}
\begin{tabular}{|l|l|p{5cm}|}
\hline
\textbf{Component} & \textbf{Version} & \textbf{Purpose} \\
\hline
Ubuntu Server & 22.04 LTS & Operating system \\
Docker & 24.0+ & Container runtime \\
Docker Compose & 2.20+ & Multi-container orchestration \\
Python & 3.11 & Application development \\
FastAPI & 0.100+ & Web API framework \\
Uvicorn & 0.23+ & ASGI server \\
PostgreSQL & 15 & Relational database \\
SQLAlchemy & 2.0+ & Async ORM \\
NFStream & 6.5+ & Network flow extraction \\
scikit-learn & 1.3+ & Random Forest model \\
joblib & 1.3+ & Model serialization \\
Jinja2 & 3.1+ & HTML templating \\
httpx/requests & - & HTTP client for agents \\
\hline
\end{tabular}
\end{table}

\subsection{Environment Variables}
Table~\ref{tab:env-vars} lists the key configuration variables for both models.

\begin{table}[H]
\centering
\caption{Environment Variables for Detection Models}
\label{tab:env-vars}
\begin{tabular}{|l|l|p{4cm}|}
\hline
\textbf{Variable} & \textbf{Default} & \textbf{Purpose} \\
\hline
\multicolumn{3}{|c|}{\textbf{SSH Model Configuration}} \\
\hline
SSH\_MODEL\_PATH & models/ssh/ssh\_lstm.joblib & Path to SSH model \\
SSH\_BRUTEFORCE\_THRESHOLD & 5 & Failed login count trigger \\
SSH\_BRUTEFORCE\_WINDOW\_SECONDS & 300 & Time window (seconds) \\
\hline
\multicolumn{3}{|c|}{\textbf{Network RF Configuration}} \\
\hline
NETWORK\_MODEL\_PATH & models/RF/random\_forest.joblib & Path to RF model \\
NETWORK\_ML\_THRESHOLD & 0.60 & Minimum confidence \\
NETWORK\_LABEL\_ALLOWLIST & DoS,DDoS,Port Scanning,Brute Force & Allowed attack labels \\
ML\_DEDUP\_WINDOW\_SECONDS & 10 & Deduplication window \\
ML\_COOLDOWN\_SECONDS\_PER\_SRC & 0 & Per-source cooldown \\
\hline
\end{tabular}
\end{table}

\section{Advanced Sensor Implementation}
\label{sec:sensor-impl}

\subsection{Network Flow Sensor (flow\_collector)}
The flow collector agent captures network traffic using NFStream:

\begin{lstlisting}[style=python,caption=Flow Collector Core Logic]
from nfstream import NFStreamer

streamer = NFStreamer(
    source=net_iface,
    idle_timeout=2,       # 2 seconds
    active_timeout=5,     # 5 seconds
    accounting_mode=1,    # IP/Port based
    statistical_analysis=True,
    n_dissections=0,      # No DPI
)

for flow in streamer:
    flow_dict = flow.to_dict()
    response = session.post(
        f"{backend_url}/api/v1/ingest/flow",
        headers={"X-API-Key": api_key},
        json={"device_id": device_id, "flow": flow_dict}
    )
\end{lstlisting}

\subsection{Auth Log Sensor (auth\_collector)}
The auth collector tails /var/log/auth.log for SSH events:

\begin{lstlisting}[style=python,caption=Auth Collector Core Logic]
def tail_auth_log(path="/var/log/auth.log"):
    with open(path, "r") as f:
        f.seek(0, 2)  # Seek to end
        while True:
            line = f.readline()
            if line:
                if is_ssh_related(line):
                    send_auth_event(line)
            else:
                time.sleep(0.1)

def send_auth_event(line):
    response = session.post(
        f"{backend_url}/api/v1/ingest/auth",
        headers={"X-API-Key": api_key},
        json={
            "device_id": device_id,
            "line": line,
            "timestamp": datetime.utcnow().isoformat()
        }
    )
\end{lstlisting}

\section{Central AI Server Implementation}
\label{sec:server-impl}

\subsection{Model Loading at Startup}
Both SSH LSTM and Network RF models are loaded during application startup:

\begin{lstlisting}[style=python,caption=Model Loading (models\_loader.py)]
from app.config import settings
import joblib

# SSH LSTM Model
ssh_lstm_model = SSHLSTMWrapper()
ssh_lstm_model.load(settings.ssh_model_path)
logger.info(f"SSH LSTM: {'LOADED' if ssh_lstm_model.loaded else 'N/A'}")

# Network RF Model  
network_ml_model = NetworkRFWrapper()
network_ml_model.load(
    settings.network_model_path,
    settings.network_features_path,
    settings.network_labels_path,
    settings.network_preprocess_path
)
logger.info(f"Network RF: {'LOADED' if network_ml_model.loaded else 'N/A'}")
\end{lstlisting}

\subsection{Auth Ingestion Route}
The auth ingestion endpoint processes events through the SSH detector:

\begin{lstlisting}[style=python,caption=Auth Ingestion (auth\_ingest.py)]
@router.post("/auth")
async def ingest_auth_event(payload: AuthEventPayload):
    # Store raw event
    event_id = await insert_raw_event(
        session, ts, payload.device_id, "auth", event_payload
    )
    
    # Run SSH LSTM detection
    detection = analyze_auth_event(payload.line, ts)
    
    if detection:
        await insert_detection(session, detection, event_id)
        bus.enqueue_alert(detection)
    
    return {"status": "accepted", "event_id": event_id}
\end{lstlisting}

\subsection{Flow Ingestion Route}
The flow ingestion endpoint processes events through the Network RF detector:

\begin{lstlisting}[style=python,caption=Flow Ingestion (flow\_ingest.py)]
@router.post("/flow")
async def ingest_flow_event(payload: FlowEventPayload):
    # Store raw event
    event_id = await insert_raw_event(
        session, ts, payload.device_id, "flow", flow_data
    )
    
    # Run Network RF detection
    detection = analyze_flow(flow_data)
    
    if detection:
        # Apply deduplication
        existing = await check_dedup(session, detection)
        if existing:
            await increment_occurrence(session, existing.id)
        else:
            await insert_detection(session, detection, event_id)
            bus.enqueue_alert(detection)
    
    return {"status": "accepted", "event_id": event_id}
\end{lstlisting}

\section{Testing and Validation Scenarios}
\label{sec:testing}

\subsection{SSH Brute Force Testing}
Lab-based SSH brute force attack simulation:

\begin{itemize}
    \item Generate 10+ failed SSH login attempts from single IP within 5 minutes
    \item Verify SSH\_BRUTE\_FORCE detection is created
    \item Verify correct source IP extraction and failed count in details
    \item Verify severity assignment (HIGH for count $\geq$ 10)
\end{itemize}

\subsection{Network Attack Testing}
Lab-based network attack simulations:

\textbf{Port Scanning:}
\begin{itemize}
    \item Generate SYN scans to multiple ports from single source
    \item Verify Port Scanning classification
    \item Verify deduplication when same scan continues
\end{itemize}

\textbf{DoS Traffic:}
\begin{itemize}
    \item Generate high-volume traffic patterns
    \item Verify DoS/DDoS classification
    \item Verify confidence score meets threshold
\end{itemize}

\subsection{API Security Testing}
\begin{itemize}
    \item Test ingestion without API key: expect 401 Unauthorized
    \item Test ingestion with invalid API key: expect 401 Unauthorized
    \item Test ingestion with valid API key: expect 200 OK
\end{itemize}

\subsection{System Resilience Testing}
\begin{itemize}
    \item Restart backend, verify sensors reconnect automatically
    \item Stop database, verify backend handles connection errors gracefully
    \item Verify Telegram alerts sent for high-severity detections
\end{itemize}

\section{Implementation Validation Results}
\label{sec:validation}

\subsection{Network RF Model Offline Metrics}
The Network RF model was evaluated on the CIC-IDS2017 test set. Table~\ref{tab:rf-offline} shows the overall metrics from \texttt{models/RF/metrics.json}:

\begin{table}[H]
\centering
\caption{Network RF Offline Evaluation Metrics}
\label{tab:rf-offline}
\begin{tabular}{|l|c|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Accuracy & 1.00 (100\%) \\
Weighted F1-Score & 1.00 (100\%) \\
Macro F1-Score & 0.96 (96\%) \\
\hline
\end{tabular}
\end{table}

\subsection{SSH Model Validation}
The SSH LSTM model validation focuses on threshold-based detection:

\begin{itemize}
    \item Threshold-based brute force detection verified with lab attacks
    \item Correctly identifies failed password patterns and invalid user attempts
    \item Source IP extraction working for standard auth.log formats
    \item Future work: evaluate sequence-based LSTM detection with labeled auth.log dataset
\end{itemize}

\section{Summary}
This chapter documented the implementation of Analytical-Intelligence, covering the software stack, environment configuration for both SSH and Network RF models, sensor agent implementations, central AI server with dual model loading and detection pipelines, testing scenarios, and validation results. The next chapter presents detailed evaluation results.

% ------------------------- الفصل 6: Results & Discussions ----------
\pagestyle{fancy}
\fancyhf{}
\lhead{Analytical Intelligence Project}
\rhead{Chapter Six: Results and Discussion}
\cfoot{\thepage}

\chapter{Results and Discussion}
\label{ch:results}

\section{Chapter Introduction}
This chapter presents the evaluation results of the Analytical-Intelligence system, focusing on performance metrics for both the SSH LSTM model and the Network RF model. We discuss model accuracy, per-class performance, hybrid detection logic, threshold tuning considerations, and dashboard outcomes.

\section{Performance Evaluation Metrics}
\label{sec:eval-metrics}
The following standard metrics are used to evaluate model performance:

\begin{itemize}
    \item \textbf{Accuracy:} Overall correct predictions / total predictions
    \item \textbf{Precision:} True positives / (True positives + False positives)
    \item \textbf{Recall:} True positives / (True positives + False negatives)
    \item \textbf{F1-Score:} Harmonic mean of precision and recall
    \item \textbf{Weighted F1:} F1-score weighted by class support
    \item \textbf{Macro F1:} Unweighted mean of per-class F1-scores
\end{itemize}

\section{Performance Results of Specialized Models}
\label{sec:model-results}

\subsection{Network RF Model Results}
The Network Random Forest model was trained and evaluated on the CIC-IDS2017 dataset. Table~\ref{tab:rf-overall} shows the overall metrics from \texttt{models/RF/metrics.json}:

\begin{table}[H]
\centering
\caption{Network RF Overall Performance}
\label{tab:rf-overall}
\begin{tabular}{|l|c|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Accuracy & 1.00 (100\%) \\
Weighted F1-Score & 1.00 (100\%) \\
Macro F1-Score & 0.96 (96\%) \\
\hline
\end{tabular}
\end{table}

\subsubsection{Per-Class Performance}
Table~\ref{tab:rf-perclass} presents the detailed per-class metrics:

\begin{table}[H]
\centering
\caption{Network RF Per-Class Metrics}
\label{tab:rf-perclass}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
\hline
Normal Traffic & 1.00 & 1.00 & 1.00 & 628,518 \\
DoS & 1.00 & 1.00 & 1.00 & 58,124 \\
DDoS & 1.00 & 1.00 & 1.00 & 38,404 \\
Port Scanning & 0.99 & 1.00 & 0.99 & 27,208 \\
Brute Force & 1.00 & 1.00 & 1.00 & 2,745 \\
Web Attacks & 0.98 & 0.97 & 0.98 & 643 \\
Bots & 0.71 & 0.83 & 0.77 & 584 \\
\hline
\end{tabular}
\end{table}

\subsubsection{Analysis of Network RF Results}
\begin{itemize}
    \item \textbf{High-Volume Classes (F1 $\geq$ 0.99):} Normal Traffic, DoS, DDoS, and Port Scanning achieve near-perfect performance with sufficient training samples and distinct flow patterns.
    \item \textbf{Brute Force (F1 = 1.00):} Perfect detection of network-level brute force attacks.
    \item \textbf{Web Attacks (F1 = 0.98):} Strong performance despite limited samples (643).
    \item \textbf{Bots (F1 = 0.77):} Lower performance due to botnet traffic mimicking normal behavior and limited training samples (584).
\end{itemize}

\textbf{Label Allowlist Justification:} The default allowlist includes DoS, DDoS, Port Scanning, and Brute Force (all with F1 $\geq$ 0.99). Web Attacks and Bots are excluded due to higher false positive potential.

\subsection{SSH LSTM Model Results}
The SSH model evaluation consists of:

\subsubsection{Threshold-Based Detection Results}
\begin{itemize}
    \item \textbf{True Positive Rate:} Successfully detects brute force attacks when failed login count exceeds threshold
    \item \textbf{Source IP Extraction:} Correctly parses IP addresses from standard auth.log formats
    \item \textbf{Token Recognition:} Correctly identifies FAILED\_PASSWORD, INVALID\_USER, and other SSH patterns
\end{itemize}

\subsubsection{SSH Model Limitations and Future Work}
\begin{itemize}
    \item Sequence-based LSTM detection requires further evaluation with labeled auth.log dataset
    \item Current evaluation is qualitative based on lab testing
    \item Future work: create labeled SSH attack dataset for quantitative metrics
\end{itemize}

\section{Discussion of Hybrid Detection Logic}
\label{sec:hybrid-discussion}

\subsection{Threshold Tuning for SSH Model}
\begin{itemize}
    \item \textbf{SSH\_BRUTEFORCE\_THRESHOLD = 5:} Balanced setting detecting attacks without excessive false positives for legitimate failed logins
    \item \textbf{SSH\_BRUTEFORCE\_WINDOW\_SECONDS = 300:} 5-minute window captures sustained attack patterns
    \item \textbf{Tuning Guidance:} Increase threshold for high-traffic environments; decrease for stricter security
\end{itemize}

\subsection{Threshold Tuning for Network RF Model}
\begin{itemize}
    \item \textbf{NETWORK\_ML\_THRESHOLD = 0.60:} Moderate confidence requirement balancing detection with false positive reduction
    \item \textbf{Lower threshold (0.3-0.5):} More sensitive, higher false positive rate
    \item \textbf{Higher threshold (0.7-0.9):} Higher confidence, may miss borderline attacks
\end{itemize}

\subsection{Deduplication and Cooldown Controls}
\begin{itemize}
    \item \textbf{ML\_DEDUP\_WINDOW\_SECONDS = 10:} Aggregates repeated detections within 10 seconds
    \item \textbf{ML\_COOLDOWN\_SECONDS\_PER\_SRC = 0:} Disabled by default for reliable detection
    \item \textbf{Effect:} Reduces alert volume by 80-95\% during sustained attacks while preserving evidence
\end{itemize}

\subsection{Severity Assignment Logic}
\begin{table}[H]
\centering
\caption{Severity Assignment Rules}
\label{tab:severity-rules}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Severity} & \textbf{SSH Model} & \textbf{Network RF Model} \\
\hline
CRITICAL & - & DDoS with confidence $\geq$ 0.9 \\
HIGH & Brute force $\geq$ 10 attempts & DoS, high confidence attacks \\
MEDIUM & Brute force 5-9 attempts & Port Scanning \\
LOW & Anomaly score near threshold & Low confidence detections \\
\hline
\end{tabular}
\end{table}

\section{Dashboard Outcomes}
\label{sec:dashboard-outcomes}
The web dashboard displays detections from both SSH and Network RF models:

\subsection{Detection Evidence Fields}
Each detection record includes:
\begin{itemize}
    \item \textbf{Common Fields:} timestamp, device\_id, model\_name, label, score, severity
    \item \textbf{SSH Detections:} src\_ip, failed\_count, token sequence, raw auth.log line
    \item \textbf{Network RF Detections:} src\_ip, dst\_ip, dst\_port, confidence, flow statistics
\end{itemize}

\subsection{Alert Notification Flow}
\begin{enumerate}
    \item Detection created by SSH or Network RF model
    \item Severity assigned based on attack type and confidence
    \item If severity $\geq$ TELEGRAM\_MIN\_SEVERITY: enqueue to NotificationBus
    \item TelegramNotifier sends alert with detection details and dashboard link
\end{enumerate}

\section{Summary}
This chapter presented the evaluation results for both detection models. The Network RF model achieves 100\% weighted F1-score and 96\% macro F1-score on CIC-IDS2017, with strong performance on DoS, DDoS, Port Scanning, and Brute Force attacks. The SSH model successfully detects brute force attacks using threshold-based logic, with LSTM sequence detection available for sophisticated attack patterns. The hybrid detection system with configurable thresholds, deduplication, and severity assignment provides practical security monitoring with quality-controlled alerts.

% ------------------------- الفصل 7: Conclusions & Recs -------------
\chapter{Conclusions and Recommendations}
% لخص ما تم والتوصيات المستقبلية 


% ------------------------- المراجع -------------------------------
\chapter{References}
\bibliographystyle{IEEEtran} % أو plain, apa, unsrt, etc.
\bibliography{references}

    % ------------------------- الملاحق -------------------------------
\appendix
\chapter{Appendix A}
% ضع أي مواد إضافية: استبيانات، شفرة، جداول مطولة…





\end{document}
