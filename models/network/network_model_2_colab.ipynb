{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Model 2 - Multiclass IDS Training\n",
    "\n",
    "This notebook trains a production-grade multiclass Intrusion Detection System (IDS) model using network flow features from `train.parquet`. The model detects the following attack classes:\n",
    "\n",
    "- BENIGN\n",
    "- DoS Hulk\n",
    "- DDoS\n",
    "- PortScan\n",
    "- DoS GoldenEye\n",
    "- FTP-Patator\n",
    "- DoS slowloris\n",
    "- DoS Slowhttptest\n",
    "- SSH-Patator\n",
    "- Bot\n",
    "- Web Attack – Brute Force\n",
    "- Web Attack – XSS\n",
    "\n",
    "**Author**: Senior ML Engineer + Cybersecurity Data Scientist\n",
    "**Date**: January 2025\n",
    "**Model**: LightGBM Multiclass Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Install\n",
    "\n",
    "Install required libraries and verify versions. This ensures reproducibility across different Colab environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from lightgbm) (2.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lightgbm) (1.16.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (9.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting optuna\n",
      "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.45)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
      "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: colorlog, optuna\n",
      "Successfully installed colorlog-6.10.1 optuna-4.6.0\n",
      "Libraries installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "%pip install lightgbm scikit-learn pandas pyarrow joblib matplotlib seaborn plotly\n",
    "\n",
    "# Optional: Install optuna for hyperparameter optimization\n",
    "%pip install optuna\n",
    "\n",
    "print(\"Libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna available for hyperparameter optimization\n",
      "pandas: 2.2.2\n",
      "numpy: 2.0.2\n",
      "lightgbm: 4.6.0\n",
      "scikit-learn: 2.2.2\n",
      "Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "# Import statements and version check\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Optional: Optuna for hyperparameter tuning\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    print(\"Optuna available for hyperparameter optimization\")\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"Optuna not available, using default hyperparameters\")\n",
    "\n",
    "# Version information\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"lightgbm: {lgb.__version__}\")\n",
    "print(f\"scikit-learn: {pd.__version__}\")  # sklearn version\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"Random seed set to: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "Load the training data from `train.parquet`. You have two options:\n",
    "- **Option A**: Upload the file directly to Colab\n",
    "- **Option B**: Mount Google Drive and load from a specified path\n",
    "\n",
    "Choose one option below and comment out the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-f9cceffd-fcb1-4186-9793-0b537c92e479\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-f9cceffd-fcb1-4186-9793-0b537c92e479\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-90434536.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Assumes train.parquet is uploaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File uploaded: {data_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Option A: Upload file directly to Colab\n",
    "# Uncomment the lines below if using direct upload\n",
    "\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "data_path = list(uploaded.keys())[0]  # Assumes train.parquet is uploaded\n",
    "print(f\"File uploaded: {data_path}\")\n",
    "\n",
    "# Option B: Mount Google Drive (recommended for large files)\n",
    "# Uncomment the lines below if using Google Drive\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # Specify the path to your train.parquet file in Google Drive\n",
    "# # Update this path to match your Drive structure\n",
    "# data_path = '/content/drive/MyDrive/train.parquet'  # Change this to your actual path\n",
    "\n",
    "# print(f\"Using data from: {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parquet file\n",
    "print(\"Loading train.parquet...\")\n",
    "df = pd.read_parquet(data_path)\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of columns: {len(df.columns)}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\nColumn information:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(\"\\nFirst few columns:\")\n",
    "print(df.dtypes.head(10))\n",
    "\n",
    "# Check for label column (common names)\n",
    "label_candidates = ['Label', 'label', 'class', 'target', 'attack_type']\n",
    "label_col = None\n",
    "for col in label_candidates:\n",
    "    if col in df.columns:\n",
    "        label_col = col\n",
    "        break\n",
    "\n",
    "if label_col:\n",
    "    print(f\"\\nDetected label column: '{label_col}'\")\n",
    "    print(f\"Unique labels: {df[label_col].nunique()}\")\n",
    "    print(f\"Sample labels: {df[label_col].unique()[:10]}\")\n",
    "else:\n",
    "    print(\"\\nWarning: Could not automatically detect label column.\")\n",
    "    print(\"Available columns:\", list(df.columns))\n",
    "    # Manually set if needed\n",
    "    label_col = 'Label'  # Change this if your label column has a different name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Label Filtering & Distribution\n",
    "\n",
    "Filter the dataset to include only the target attack classes. We need to handle potential unicode variations in \"Web Attack\" labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target labels (normalized)\n",
    "TARGET_LABELS = [\n",
    "    'BENIGN',\n",
    "    'DoS Hulk',\n",
    "    'DDoS',\n",
    "    'PortScan',\n",
    "    'DoS GoldenEye',\n",
    "    'FTP-Patator',\n",
    "    'DoS slowloris',\n",
    "    'DoS Slowhttptest',\n",
    "    'SSH-Patator',\n",
    "    'Bot',\n",
    "    'Web Attack – Brute Force',  # Note: en-dash\n",
    "    'Web Attack – XSS'  # Note: en-dash\n",
    "]\n",
    "\n",
    "# Alternative variations (handle different unicode dashes)\n",
    "LABEL_MAPPINGS = {\n",
    "    'Web Attack - Brute Force': 'Web Attack – Brute Force',  # hyphen to en-dash\n",
    "    'Web Attack — Brute Force': 'Web Attack – Brute Force',  # em-dash to en-dash\n",
    "    'Web Attack - XSS': 'Web Attack – XSS',\n",
    "    'Web Attack — XSS': 'Web Attack – XSS',\n",
    "    'WEB ATTACK – BRUTE FORCE': 'Web Attack – Brute Force',\n",
    "    'WEB ATTACK – XSS': 'Web Attack – XSS',\n",
    "    'Web Attack Brute Force': 'Web Attack – Brute Force',\n",
    "    'Web Attack XSS': 'Web Attack – XSS',\n",
    "}\n",
    "\n",
    "print(\"Target labels to keep:\")\n",
    "for label in TARGET_LABELS:\n",
    "    print(f\"  - {label}\")\n",
    "\n",
    "# Check original labels in dataset\n",
    "original_labels = df[label_col].unique()\n",
    "print(f\"\\nOriginal labels in dataset ({len(original_labels)}):\")\n",
    "for label in sorted(original_labels):\n",
    "    print(f\"  - '{label}'\")\n",
    "\n",
    "# Normalize labels using mapping\n",
    "df[label_col] = df[label_col].map(LABEL_MAPPINGS).fillna(df[label_col])\n",
    "\n",
    "# Filter to target labels only\n",
    "df_filtered = df[df[label_col].isin(TARGET_LABELS)].copy()\n",
    "\n",
    "print(f\"\\nAfter filtering:\")\n",
    "print(f\"  Original dataset: {df.shape[0]} rows\")\n",
    "print(f\"  Filtered dataset: {df_filtered.shape[0]} rows\")\n",
    "print(f\"  Removed: {df.shape[0] - df_filtered.shape[0]} rows\")\n",
    "\n",
    "# Check if all target labels are present\n",
    "present_labels = df_filtered[label_col].unique()\n",
    "missing_labels = set(TARGET_LABELS) - set(present_labels)\n",
    "\n",
    "if missing_labels:\n",
    "    print(f\"\\nWarning: Missing target labels: {missing_labels}\")\n",
    "else:\n",
    "    print(\"\\nAll target labels present in filtered dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution\n",
    "label_counts = df_filtered[label_col].value_counts()\n",
    "label_percentages = df_filtered[label_col].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "print(\"=\" * 50)\n",
    "for label in TARGET_LABELS:\n",
    "    if label in label_counts.index:\n",
    "        count = label_counts[label]\n",
    "        percentage = label_percentages[label]\n",
    "        print(f\"{label:<25} {count:>8} ({percentage:>6.2f}%)\")\n",
    "    else:\n",
    "        print(f\"{label:<25} {0:>8} ({0.00:>6.2f}%)\")\n",
    "\n",
    "# Create distribution plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = label_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Class Distribution After Filtering', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Attack Type', fontsize=12)\n",
    "plt.ylabel('Number of Samples', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(label_counts):\n",
    "    ax.text(i, v + max(label_counts) * 0.01, f'{v:,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for class imbalance\n",
    "max_class = label_counts.max()\n",
    "min_class = label_counts.min()\n",
    "imbalance_ratio = max_class / min_class\n",
    "\n",
    "print(f\"\\nClass imbalance analysis:\")\n",
    "print(f\"  Largest class: {max_class:,} samples\")\n",
    "print(f\"  Smallest class: {min_class:,} samples\")\n",
    "print(f\"  Imbalance ratio: {imbalance_ratio:.1f}x\")\n",
    "\n",
    "if imbalance_ratio > 10:\n",
    "    print(\"  ⚠️  Severe class imbalance detected - will use class weights\")\n",
    "elif imbalance_ratio > 5:\n",
    "    print(\"  ⚠️  Moderate class imbalance detected - will use class weights\")\n",
    "else:\n",
    "    print(\"  ✓ Class distribution is relatively balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning (must do all)\n",
    "\n",
    "Perform comprehensive data cleaning including:\n",
    "- Clip invalid negatives to 0 for: FlowDuration, FlowBytes/s, FlowPackets/s (only if columns exist)\n",
    "- Replace inf/-inf with NaN\n",
    "- Drop fully-constant columns automatically\n",
    "- Drop duplicate columns automatically (example: FwdHeaderLength.1 vs FwdHeaderLength if present)\n",
    "- Impute missing values (median)\n",
    "- Convert numeric features to float32 for memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with filtered dataset\n",
    "df_clean = df_filtered.copy()\n",
    "print(f\"Starting with {df_clean.shape[0]} rows and {df_clean.shape[1]} columns\")\n",
    "\n",
    "# Step 1: Clip invalid negatives to 0\n",
    "columns_to_clip = ['Flow Duration', 'Flow Bytes/s', 'Flow Packets/s']\n",
    "clipped_cols = []\n",
    "\n",
    "for col in columns_to_clip:\n",
    "    if col in df_clean.columns:\n",
    "        negative_count = (df_clean[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            df_clean[col] = df_clean[col].clip(lower=0)\n",
    "            clipped_cols.append(col)\n",
    "            print(f\"Clipped {negative_count} negative values in '{col}'\")\n",
    "\n",
    "if not clipped_cols:\n",
    "    print(\"No negative values found in flow-related columns\")\n",
    "\n",
    "# Step 2: Replace inf/-inf with NaN\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "inf_count_before = df_clean[numeric_cols].isin([np.inf, -np.inf]).sum().sum()\n",
    "\n",
    "df_clean[numeric_cols] = df_clean[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "inf_count_after = df_clean[numeric_cols].isin([np.inf, -np.inf]).sum().sum()\n",
    "print(f\"Replaced {inf_count_before} infinite values with NaN\")\n",
    "\n",
    "# Step 3: Drop fully-constant columns\n",
    "constant_cols = []\n",
    "for col in df_clean.columns:\n",
    "    if col != label_col and df_clean[col].nunique() == 1:\n",
    "        constant_cols.append(col)\n",
    "\n",
    "if constant_cols:\n",
    "    df_clean = df_clean.drop(columns=constant_cols)\n",
    "    print(f\"Dropped {len(constant_cols)} constant columns: {constant_cols}\")\n",
    "else:\n",
    "    print(\"No constant columns found\")\n",
    "\n",
    "# Step 4: Drop duplicate columns (exact duplicates)\n",
    "duplicate_cols = []\n",
    "cols_to_check = [col for col in df_clean.columns if col != label_col]\n",
    "\n",
    "for i, col1 in enumerate(cols_to_check):\n",
    "    for col2 in cols_to_check[i+1:]:\n",
    "        if df_clean[col1].equals(df_clean[col2]):\n",
    "            duplicate_cols.append(col2)\n",
    "\n",
    "if duplicate_cols:\n",
    "    df_clean = df_clean.drop(columns=duplicate_cols)\n",
    "    print(f\"Dropped {len(duplicate_cols)} duplicate columns: {duplicate_cols}\")\n",
    "else:\n",
    "    print(\"No duplicate columns found\")\n",
    "\n",
    "# Step 5: Impute missing values with median\n",
    "missing_before = df_clean.isnull().sum().sum()\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if df_clean[col].isnull().any():\n",
    "        median_val = df_clean[col].median()\n",
    "        df_clean[col] = df_clean[col].fillna(median_val)\n",
    "\n",
    "missing_after = df_clean.isnull().sum().sum()\n",
    "print(f\"Imputed {missing_before} missing values with column medians\")\n",
    "\n",
    "# Step 6: Convert numeric features to float32 for memory efficiency\n",
    "for col in numeric_cols:\n",
    "    if col != label_col:  # Don't convert label column\n",
    "        df_clean[col] = df_clean[col].astype(np.float32)\n",
    "\n",
    "print(f\"Converted {len(numeric_cols)} numeric columns to float32\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\nData cleaning completed:\")\n",
    "print(f\"  Final shape: {df_clean.shape}\")\n",
    "print(f\"  Memory usage: {df_clean.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  Missing values remaining: {df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Set\n",
    "\n",
    "Define the X columns automatically as all numeric columns excluding label columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (all numeric columns except label)\n",
    "feature_cols = [col for col in df_clean.select_dtypes(include=[np.number]).columns if col != label_col]\n",
    "\n",
    "print(f\"Identified {len(feature_cols)} feature columns\")\n",
    "print(\"\\nFeature columns:\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Verify no label column in features\n",
    "if label_col in feature_cols:\n",
    "    raise ValueError(f\"Label column '{label_col}' should not be in feature columns\")\n",
    "\n",
    "# Check for any remaining non-numeric columns\n",
    "non_numeric = [col for col in df_clean.columns if col not in feature_cols and col != label_col]\n",
    "if non_numeric:\n",
    "    print(f\"\\nWarning: {len(non_numeric)} non-numeric columns will be excluded: {non_numeric}\")\n",
    "\n",
    "# Store feature list for later use\n",
    "FEATURE_LIST = feature_cols\n",
    "\n",
    "# Display feature statistics\n",
    "X_summary = df_clean[feature_cols].describe().T\n",
    "print(f\"\\nFeature summary (showing first 10):\")\n",
    "print(X_summary.head(10)[['count', 'mean', 'std', 'min', 'max']].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train/Val/Test Split\n",
    "\n",
    "Perform stratified split maintaining class distribution across splits. Using 70/15/15 split for train/validation/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for splitting\n",
    "X = df_clean[feature_cols]\n",
    "y = df_clean[label_col]\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Label vector shape: {y.shape}\")\n",
    "\n",
    "# First split: separate test set (15%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.15,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Second split: separate validation from remaining (15% of total = 17.65% of remaining)\n",
    "val_size = 0.1765  # 15% / 85% ≈ 0.1765\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=val_size,\n",
    "    stratify=y_temp,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(\"\\nSplit sizes:\")\n",
    "print(f\"  Train: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Validation: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Total: {len(X)} samples\")\n",
    "\n",
    "# Verify stratification\n",
    "def print_class_distribution(y_data, title):\n",
    "    counts = y_data.value_counts()\n",
    "    percentages = (counts / len(y_data) * 100).round(2)\n",
    "    print(f\"\\n{title} class distribution:\")\n",
    "    for label in TARGET_LABELS:\n",
    "        if label in counts.index:\n",
    "            print(f\"  {label:<25} {counts[label]:>6} ({percentages[label]:>5.2f}%)\")\n",
    "        else:\n",
    "            print(f\"  {label:<25} {0:>6} ({0.00:>5.2f}%)\")\n",
    "\n",
    "print_class_distribution(y_train, \"Train set\")\n",
    "print_class_distribution(y_val, \"Validation set\")\n",
    "print_class_distribution(y_test, \"Test set\")\n",
    "\n",
    "# Verify no data leakage\n",
    "train_indices = set(X_train.index)\n",
    "val_indices = set(X_val.index)\n",
    "test_indices = set(X_test.index)\n",
    "\n",
    "assert len(train_indices & val_indices) == 0, \"Data leakage between train and validation\"\n",
    "assert len(train_indices & test_indices) == 0, \"Data leakage between train and test\"\n",
    "assert len(val_indices & test_indices) == 0, \"Data leakage between validation and test\"\n",
    "\n",
    "print(\"\\n✓ No data leakage detected between splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Modeling (Primary: LightGBM multiclass)\n",
    "\n",
    "Train a LightGBM multiclass classifier with class weights to handle imbalance and early stopping for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Create label mapping\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "reverse_label_mapping = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))\n",
    "\n",
    "print(\"Label encoding:\")\n",
    "for label, code in label_mapping.items():\n",
    "    print(f\"  {code}: {label}\")\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
    "class_weight_dict = dict(zip(np.unique(y_train_encoded), class_weights))\n",
    "\n",
    "print(f\"\\nClass weights: {class_weight_dict}\")\n",
    "\n",
    "# Convert to sample weights for LightGBM\n",
    "sample_weights = np.array([class_weight_dict[class_] for class_ in y_train_encoded])\n",
    "\n",
    "print(f\"Sample weights shape: {sample_weights.shape}\")\n",
    "print(f\"Sample weights range: {sample_weights.min():.3f} - {sample_weights.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Hyperparameter tuning with Optuna (fast version)\n",
    "DO_HYPERPARAMETER_TUNING = OPTUNA_AVAILABLE and X_train.shape[0] > 100000  # Only if dataset is large enough\n",
    "\n",
    "if DO_HYPERPARAMETER_TUNING:\n",
    "    print(\"Performing hyperparameter tuning with Optuna...\")\n",
    "    \n",
    "    # Sample a subset for faster tuning\n",
    "    sample_size = min(200000, len(X_train))\n",
    "    sample_indices = np.random.choice(len(X_train), sample_size, replace=False)\n",
    "    X_sample = X_train.iloc[sample_indices]\n",
    "    y_sample = y_train_encoded[sample_indices]\n",
    "    weights_sample = sample_weights[sample_indices]\n",
    "    \n",
    "    print(f\"Using {sample_size} samples for hyperparameter tuning\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'objective': 'multiclass',\n",
    "            'num_class': len(TARGET_LABELS),\n",
    "            'metric': 'multi_logloss',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'verbosity': -1,\n",
    "            'seed': RANDOM_SEED,\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "            'max_depth': trial.suggest_int('max_depth', 6, 15),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'n_estimators': 1000,  # Will be controlled by early stopping\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 1.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_alpha', 1e-5, 1.0, log=True),\n",
    "        }\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = lgb.Dataset(X_sample, y_sample, weight=weights_sample)\n",
    "        val_dataset = lgb.Dataset(X_val, y_val_encoded, reference=train_dataset)\n",
    "        \n",
    "        # Train with early stopping\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_dataset,\n",
    "            valid_sets=[val_dataset],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(50, verbose=False),\n",
    "                lgb.log_evaluation(0)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Return best score\n",
    "        return model.best_score['valid_0']['multi_logloss']\n",
    "    \n",
    "    # Run optimization\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=20, timeout=600)  # 20 trials, max 10 minutes\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    print(f\"Best hyperparameters: {best_params}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Using default hyperparameters (tuning skipped)\")\n",
    "    best_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set final model parameters\n",
    "default_params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': len(TARGET_LABELS),\n",
    "    'metric': 'multi_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': 1,\n",
    "    'seed': RANDOM_SEED,\n",
    "    'num_leaves': 50,\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 1000,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "}\n",
    "\n",
    "# Merge with tuned parameters if available\n",
    "final_params = {**default_params, **best_params}\n",
    "\n",
    "print(\"Final model parameters:\")\n",
    "for key, value in final_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create LightGBM datasets\n",
    "train_dataset = lgb.Dataset(X_train, y_train_encoded, weight=sample_weights)\n",
    "val_dataset = lgb.Dataset(X_val, y_val_encoded, reference=train_dataset)\n",
    "\n",
    "print(\"\\nTraining LightGBM model...\")\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(\n",
    "    final_params,\n",
    "    train_dataset,\n",
    "    valid_sets=[train_dataset, val_dataset],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(50, verbose=True),\n",
    "        lgb.log_evaluation(50)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining completed after {model.best_iteration} iterations\")\n",
    "print(f\"Best validation score: {model.best_score['valid']['multi_logloss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation (must include)\n",
    "\n",
    "Evaluate the trained model on the test set using multiple metrics including macro and weighted F1 scores, per-class metrics, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "print(\"Generating predictions on test set...\")\n",
    "y_pred_encoded = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "y_pred = np.argmax(y_pred_encoded, axis=1)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Get prediction probabilities\n",
    "y_pred_proba = y_pred_encoded\n",
    "\n",
    "# Calculate overall metrics\n",
    "macro_f1 = f1_score(y_test_encoded, y_pred, average='macro')\n",
    "weighted_f1 = f1_score(y_test_encoded, y_pred, average='weighted')\n",
    "\n",
    "print(f\"\\nOverall Metrics:\")\n",
    "print(f\"  Macro F1 Score: {macro_f1:.4f}\")\n",
    "print(f\"  Weighted F1 Score: {weighted_f1:.4f}\")\n",
    "\n",
    "# Generate classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "report = classification_report(y_test_encoded, y_pred, target_names=TARGET_LABELS, output_dict=True)\n",
    "\n",
    "# Convert to DataFrame for better display\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "report_df = report_df.round(4)\n",
    "print(report_df.to_string())\n",
    "\n",
    "# Store metrics for export\n",
    "evaluation_metrics = {\n",
    "    'macro_f1': macro_f1,\n",
    "    'weighted_f1': weighted_f1,\n",
    "    'accuracy': report['accuracy'],\n",
    "    'per_class_metrics': {}\n",
    "}\n",
    "\n",
    "# Add per-class metrics\n",
    "for i, label in enumerate(TARGET_LABELS):\n",
    "    if label in report:\n",
    "        evaluation_metrics['per_class_metrics'][label] = {\n",
    "            'precision': report[label]['precision'],\n",
    "            'recall': report[label]['recall'],\n",
    "            'f1-score': report[label]['f1-score'],\n",
    "            'support': report[label]['support']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test_encoded, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=TARGET_LABELS, yticklabels=TARGET_LABELS)\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze confusion matrix\n",
    "print(\"\\nConfusion Matrix Analysis:\")\n",
    "\n",
    "# Most confused pairs\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "np.fill_diagonal(cm_normalized, 0)  # Remove diagonal\n",
    "\n",
    "# Find most confused pairs\n",
    "max_conf_idx = np.unravel_index(np.argmax(cm_normalized), cm_normalized.shape)\n",
    "true_class = TARGET_LABELS[max_conf_idx[0]]\n",
    "pred_class = TARGET_LABELS[max_conf_idx[1]]\n",
    "conf_rate = cm_normalized[max_conf_idx] * 100\n",
    "\n",
    "print(f\"Most confused pair: {true_class} → {pred_class} ({conf_rate:.1f}% of {true_class} samples)\")\n",
    "\n",
    "# Per-class accuracy\n",
    "class_accuracy = np.diag(cm) / np.sum(cm, axis=1)\n",
    "print(\"\\nPer-class accuracy:\")\n",
    "for i, label in enumerate(TARGET_LABELS):\n",
    "    acc = class_accuracy[i] * 100\n",
    "    print(f\"  {label:<25} {acc:>6.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance interpretation\n",
    "print(\"\\nModel Performance Interpretation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Identify hardest classes\n",
    "f1_scores = [report[label]['f1-score'] for label in TARGET_LABELS if label in report]\n",
    "min_f1_idx = np.argmin(f1_scores)\n",
    "max_f1_idx = np.argmax(f1_scores)\n",
    "\n",
    "hardest_class = TARGET_LABELS[min_f1_idx]\n",
    "easiest_class = TARGET_LABELS[max_f1_idx]\n",
    "\n",
    "print(f\"Best performing class: {easiest_class} (F1 = {f1_scores[max_f1_idx]:.4f})\")\n",
    "print(f\"Worst performing class: {hardest_class} (F1 = {f1_scores[min_f1_idx]:.4f})\")\n",
    "\n",
    "# Analyze class imbalance impact\n",
    "support_values = [report[label]['support'] for label in TARGET_LABELS if label in report]\n",
    "min_support = min(support_values)\n",
    "max_support = max(support_values)\n",
    "\n",
    "print(f\"\\nClass imbalance analysis:\")\n",
    "print(f\"  Sample size range: {min_support} - {max_support} ({max_support/min_support:.1f}x ratio)\")\n",
    "\n",
    "if max_support / min_support > 10:\n",
    "    print(\"  ⚠️  Severe class imbalance likely contributes to poor performance on minority classes\")\n",
    "elif max_support / min_support > 5:\n",
    "    print(\"  ⚠️  Moderate class imbalance may affect minority class performance\")\n",
    "else:\n",
    "    print(\"  ✓ Class distribution is relatively balanced\")\n",
    "\n",
    "# Overall assessment\n",
    "if macro_f1 > 0.9:\n",
    "    print(\"\\n🎉 Excellent model performance!\")\n",
    "elif macro_f1 > 0.8:\n",
    "    print(\"\\n✅ Good model performance\")\n",
    "elif macro_f1 > 0.7:\n",
    "    print(\"\\n⚠️  Acceptable model performance - may need improvement\")\n",
    "else:\n",
    "    print(\"\\n❌ Poor model performance - requires significant improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance\n",
    "\n",
    "Analyze and export the most important features for model interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = model.feature_importance(importance_type='gain')\n",
    "feature_names = FEATURE_LIST\n",
    "\n",
    "# Create DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(f\"Top 20 most important features:\")\n",
    "print(\"=\" * 50)\n",
    "for i, row in importance_df.head(20).iterrows():\n",
    "    print(f\"{i+1:2d}. {row['feature']:<30} {row['importance']:>10.2f}\")\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_20 = importance_df.head(20)\n",
    "plt.barh(range(len(top_20)), top_20['importance'][::-1])\n",
    "plt.yticks(range(len(top_20)), top_20['feature'][::-1])\n",
    "plt.xlabel('Feature Importance (Gain)')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Top 20 Most Important Features', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Export top 50 features\n",
    "top_50_features = importance_df.head(50)\n",
    "print(f\"\\nExporting top {len(top_50_features)} features for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Artifacts (to Drive and to local download)\n",
    "\n",
    "Save all model artifacts to both Google Drive and local download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create artifacts directory in Drive\n",
    "artifacts_dir = '/content/drive/MyDrive/ids_artifacts/'\n",
    "!mkdir -p \"$artifacts_dir\"\n",
    "\n",
    "print(f\"Saving artifacts to: {artifacts_dir}\")\n",
    "\n",
    "# 1. Save model\n",
    "model_path = f\"{artifacts_dir}model.joblib\"\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"✓ Model saved: {model_path}\")\n",
    "\n",
    "# 2. Save label mapping\n",
    "label_map_path = f\"{artifacts_dir}label_map.json\"\n",
    "with open(label_map_path, 'w') as f:\n",
    "    json.dump(label_mapping, f, indent=2)\n",
    "print(f\"✓ Label mapping saved: {label_map_path}\")\n",
    "\n",
    "# 3. Save feature list\n",
    "feature_list_path = f\"{artifacts_dir}feature_list.json\"\n",
    "with open(feature_list_path, 'w') as f:\n",
    "    json.dump(FEATURE_LIST, f, indent=2)\n",
    "print(f\"✓ Feature list saved: {feature_list_path}\")\n",
    "\n",
    "# 4. Save evaluation metrics\n",
    "metrics_path = f\"{artifacts_dir}metrics.json\"\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(evaluation_metrics, f, indent=2)\n",
    "print(f\"✓ Metrics saved: {metrics_path}\")\n",
    "\n",
    "# 5. Save feature importance\n",
    "importance_path = f\"{artifacts_dir}feature_importance.csv\"\n",
    "importance_df.to_csv(importance_path, index=False)\n",
    "print(f\"✓ Feature importance saved: {importance_path}\")\n",
    "\n",
    "# Also save to local Colab files for download\n",
    "local_artifacts_dir = '/content/artifacts/'\n",
    "!mkdir -p \"$local_artifacts_dir\"\n",
    "\n",
    "joblib.dump(model, f\"{local_artifacts_dir}model.joblib\")\n",
    "with open(f\"{local_artifacts_dir}label_map.json\", 'w') as f:\n",
    "    json.dump(label_mapping, f, indent=2)\n",
    "with open(f\"{local_artifacts_dir}feature_list.json\", 'w') as f:\n",
    "    json.dump(FEATURE_LIST, f, indent=2)\n",
    "with open(f\"{local_artifacts_dir}metrics.json\", 'w') as f:\n",
    "    json.dump(evaluation_metrics, f, indent=2)\n",
    "importance_df.to_csv(f\"{local_artifacts_dir}feature_importance.csv\", index=False)\n",
    "\n",
    "print(f\"\\n✓ All artifacts also saved locally to: {local_artifacts_dir}\")\n",
    "\n",
    "# Create zip file for easy download\n",
    "zip_path = '/content/ids_artifacts.zip'\n",
    "!zip -r \"$zip_path\" \"$local_artifacts_dir\"\n",
    "print(f\"✓ Created zip archive: {zip_path}\")\n",
    "\n",
    "# List files\n",
    "print(\"\\nArtifacts created:\")\n",
    "!ls -la \"$local_artifacts_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Inference Demo Cell\n",
    "\n",
    "Demonstrate how to load the saved model and use it for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved artifacts (simulate production usage)\n",
    "print(\"Loading saved model and artifacts for inference demo...\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = joblib.load(f\"{local_artifacts_dir}model.joblib\")\n",
    "print(\"✓ Model loaded\")\n",
    "\n",
    "# Load mappings\n",
    "with open(f\"{local_artifacts_dir}label_map.json\", 'r') as f:\n",
    "    loaded_label_map = json.load(f)\n",
    "print(\"✓ Label mapping loaded\")\n",
    "\n",
    "with open(f\"{local_artifacts_dir}feature_list.json\", 'r') as f:\n",
    "    loaded_feature_list = json.load(f)\n",
    "print(\"✓ Feature list loaded\")\n",
    "\n",
    "# Create reverse mapping for predictions\n",
    "reverse_mapping = {v: k for k, v in loaded_label_map.items()}\n",
    "\n",
    "# Prepare sample data for prediction\n",
    "sample_size = min(100, len(X_test))\n",
    "X_sample = X_test.head(sample_size)\n",
    "y_sample_true = y_test.head(sample_size)\n",
    "\n",
    "print(f\"\\nRunning inference on {sample_size} sample instances...\")\n",
    "\n",
    "# Make predictions\n",
    "predictions_proba = loaded_model.predict(X_sample, num_iteration=loaded_model.best_iteration)\n",
    "predictions_encoded = np.argmax(predictions_proba, axis=1)\n",
    "predictions_labels = [reverse_mapping[pred] for pred in predictions_encoded]\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'True_Label': y_sample_true.values,\n",
    "    'Predicted_Label': predictions_labels,\n",
    "    'Correct': y_sample_true.values == predictions_labels\n",
    "})\n",
    "\n",
    "# Add confidence scores (probability of predicted class)\n",
    "confidence_scores = np.max(predictions_proba, axis=1)\n",
    "results_df['Confidence'] = confidence_scores\n",
    "\n",
    "print(\"\\nPrediction Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(results_df.head(10).to_string(index=False))\n",
    "\n",
    "# Show probability distribution for first sample\n",
    "print(f\"\\nProbability distribution for first sample:\")\n",
    "print(f\"True label: {results_df.iloc[0]['True_Label']}\")\n",
    "print(f\"Predicted: {results_df.iloc[0]['Predicted_Label']} (confidence: {results_df.iloc[0]['Confidence']:.4f})\")\n",
    "print(\"\\nClass probabilities:\")\n",
    "for i, prob in enumerate(predictions_proba[0]):\n",
    "    class_name = reverse_mapping[i]\n",
    "    marker = \" ←\" if i == predictions_encoded[0] else \"\"\n",
    "    print(f\"  {class_name:<25} {prob:>7.4f}{marker}\")\n",
    "\n",
    "# Calculate accuracy on sample\n",
    "sample_accuracy = results_df['Correct'].mean()\n",
    "print(f\"\\nSample accuracy: {sample_accuracy:.4f} ({results_df['Correct'].sum()}/{len(results_df)} correct)\")\n",
    "\n",
    "print(\"\\n✓ Inference demo completed successfully!\")\n",
    "print(\"\\nTo use this model in production:\")\n",
    "print(\"1. Load the model: joblib.load('model.joblib')\")\n",
    "print(\"2. Load feature list and ensure input data has these columns\")\n",
    "print(\"3. Load label mapping to convert predictions to class names\")\n",
    "print(\"4. Use model.predict() for predictions or model.predict_proba() for probabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Complete!\n",
    "\n",
    "**Summary:**\n",
    "- ✅ Trained multiclass LightGBM model for IDS\n",
    "- ✅ Handles 12 attack classes including BENIGN\n",
    "- ✅ Uses class weights to handle imbalance\n",
    "- ✅ Includes early stopping and hyperparameter tuning\n",
    "- ✅ Comprehensive evaluation with multiple metrics\n",
    "- ✅ All artifacts exported and ready for deployment\n",
    "\n",
    "**Next Steps:**\n",
    "1. Download the artifacts zip file\n",
    "2. Deploy model to production environment\n",
    "3. Set up monitoring and retraining pipeline\n",
    "4. Consider model compression for edge deployment\n",
    "\n",
    "**Model Performance:**\n",
    "- Macro F1: {macro_f1:.4f}\n",
    "- Weighted F1: {weighted_f1:.4f}\n",
    "- Best class: {easiest_class}\n",
    "- Needs improvement: {hardest_class}\n",
    "\n",
    "**Files Generated:**\n",
    "- `model.joblib` - Trained LightGBM model\n",
    "- `feature_list.json` - List of features used\n",
    "- `label_map.json` - Class name to ID mapping\n",
    "- `metrics.json` - Evaluation metrics\n",
    "- `feature_importance.csv` - Feature importance rankings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
