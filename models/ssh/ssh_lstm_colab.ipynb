{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è SSH Brute Force Detection - Minimal LSTM Pipeline\n",
    "\n",
    "A minimal end-to-end LSTM training pipeline for SSH brute-force detection.\n",
    "\n",
    "**Workflow:**\n",
    "1. Install & Setup\n",
    "2. Upload SSH.log\n",
    "3. Parse + Build Sequences\n",
    "4. Train LSTM\n",
    "5. Evaluate\n",
    "6. Export ssh_lstm.joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: INSTALL & SETUP\n",
    "# =============================================================================\n",
    "%pip install -q pandas numpy python-dateutil tensorflow joblib\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil import parser as dateparser\n",
    "from typing import Optional\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "WINDOW_SIZE = 20        # Events per window\n",
    "STRIDE = 5              # Sliding window stride\n",
    "TIME_WINDOW_SEC = 120   # 2 minutes for labeling\n",
    "FAIL_THRESHOLD = 10     # Min suspicious events for attack label\n",
    "\n",
    "# LSTM params\n",
    "EMBEDDING_DIM = 32\n",
    "LSTM_UNITS = 64\n",
    "DROPOUT = 0.3\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 256\n",
    "PATIENCE = 5\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"Config: WINDOW_SIZE={WINDOW_SIZE}, STRIDE={STRIDE}, FAIL_THRESHOLD={FAIL_THRESHOLD}\")\n",
    "print(\"‚úÖ Setup complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: UPLOAD SSH.log\n",
    "# =============================================================================\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üìÅ Please upload your SSH.log file:\")\n",
    "uploaded = files.upload()\n",
    "log_path = list(uploaded.keys())[0]\n",
    "print(f\"‚úÖ Loaded: {log_path} ({len(uploaded[log_path]):,} bytes)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: PARSE + BUILD SEQUENCES\n",
    "# =============================================================================\n",
    "\n",
    "# Event taxonomy\n",
    "EVENT_TYPES = [\n",
    "    'PAD', 'FAILED_PASSWORD', 'INVALID_USER', 'ACCEPTED_PASSWORD',\n",
    "    'ACCEPTED_PUBLICKEY', 'DISCONNECT', 'REVERSE_DNS_FAIL',\n",
    "    'PAM_AUTH_FAILURE', 'CONNECTION_CLOSED', 'SESSION_OPENED',\n",
    "    'SESSION_CLOSED', 'OTHER'\n",
    "]\n",
    "token2id = {t: i for i, t in enumerate(EVENT_TYPES)}\n",
    "VOCAB_SIZE = len(token2id)\n",
    "\n",
    "# Suspicious events for attack labeling\n",
    "SUSPICIOUS_EVENTS = {'FAILED_PASSWORD', 'INVALID_USER', 'PAM_AUTH_FAILURE', 'REVERSE_DNS_FAIL'}\n",
    "\n",
    "# IP extraction patterns\n",
    "IP_PATTERNS = [\n",
    "    re.compile(r'from\\s+((?:\\d{1,3}\\.){3}\\d{1,3})'),\n",
    "    re.compile(r'by\\s+((?:\\d{1,3}\\.){3}\\d{1,3})'),\n",
    "    re.compile(r'rhost=((?:\\d{1,3}\\.){3}\\d{1,3})'),\n",
    "    re.compile(r'\\[((?:\\d{1,3}\\.){3}\\d{1,3})\\]'),\n",
    "]\n",
    "\n",
    "def extract_ip(line: str) -> Optional[str]:\n",
    "    for pattern in IP_PATTERNS:\n",
    "        m = pattern.search(line)\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "    return None\n",
    "\n",
    "def classify_event(line: str) -> str:\n",
    "    lower = line.lower()\n",
    "    if 'failed password' in lower: return 'FAILED_PASSWORD'\n",
    "    if 'invalid user' in lower and 'failed' not in lower: return 'INVALID_USER'\n",
    "    if 'accepted publickey' in lower: return 'ACCEPTED_PUBLICKEY'\n",
    "    if 'accepted password' in lower: return 'ACCEPTED_PASSWORD'\n",
    "    if 'authentication failure' in lower or ('pam_unix' in lower and 'auth' in lower): return 'PAM_AUTH_FAILURE'\n",
    "    if 'possible break-in' in lower or 'reverse mapping' in lower: return 'REVERSE_DNS_FAIL'\n",
    "    if 'disconnect' in lower: return 'DISCONNECT'\n",
    "    if 'connection closed' in lower: return 'CONNECTION_CLOSED'\n",
    "    if 'session opened' in lower: return 'SESSION_OPENED'\n",
    "    if 'session closed' in lower: return 'SESSION_CLOSED'\n",
    "    return 'OTHER'\n",
    "\n",
    "def infer_year(lines):\n",
    "    current_year = datetime.now().year\n",
    "    for line in lines[:50]:\n",
    "        parts = line.split()\n",
    "        if len(parts) >= 3:\n",
    "            try:\n",
    "                ts = dateparser.parse(f\"{parts[0]} {parts[1]} {parts[2]} {current_year}\")\n",
    "                if ts and ts > datetime.now():\n",
    "                    return current_year - 1\n",
    "            except: pass\n",
    "    return current_year\n",
    "\n",
    "# === PARSE LOG FILE ===\n",
    "print(\"üîç Parsing log file...\")\n",
    "with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    raw_lines = f.readlines()\n",
    "\n",
    "year = infer_year(raw_lines)\n",
    "print(f\"üìÖ Using year: {year}\")\n",
    "\n",
    "events = []\n",
    "for line in raw_lines:\n",
    "    line = line.strip()\n",
    "    if not line: continue\n",
    "    parts = line.split()\n",
    "    if len(parts) < 4: continue\n",
    "    try:\n",
    "        timestamp = dateparser.parse(f\"{parts[0]} {parts[1]} {parts[2]} {year}\")\n",
    "    except: continue\n",
    "    ip = extract_ip(line)\n",
    "    if not ip: continue\n",
    "    events.append({'timestamp': timestamp, 'src_ip': ip, 'event_type': classify_event(line)})\n",
    "\n",
    "df_events = pd.DataFrame(events).sort_values(['src_ip', 'timestamp']).reset_index(drop=True)\n",
    "print(f\"‚úÖ Parsed {len(df_events):,} events from {df_events['src_ip'].nunique():,} IPs\")\n",
    "print(f\"\\nüìà Event distribution:\\n{df_events['event_type'].value_counts()}\")\n",
    "\n",
    "# === BUILD SEQUENCES & LABELS ===\n",
    "print(f\"\\nüîÑ Building sequences (WINDOW={WINDOW_SIZE}, STRIDE={STRIDE})...\")\n",
    "\n",
    "sequences, labels = [], []\n",
    "for ip, group in df_events.groupby('src_ip'):\n",
    "    group = group.reset_index(drop=True)\n",
    "    n = len(group)\n",
    "    if n < WINDOW_SIZE: continue\n",
    "    \n",
    "    for start in range(0, n - WINDOW_SIZE + 1, STRIDE):\n",
    "        window = group.iloc[start:start + WINDOW_SIZE]\n",
    "        seq = [token2id[e] for e in window['event_type']]\n",
    "        \n",
    "        # Weak labeling\n",
    "        time_span = (window['timestamp'].max() - window['timestamp'].min()).total_seconds()\n",
    "        suspicious_count = window['event_type'].isin(SUSPICIOUS_EVENTS).sum()\n",
    "        label = 1 if (time_span <= TIME_WINDOW_SEC and suspicious_count >= FAIL_THRESHOLD) else 0\n",
    "        \n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "\n",
    "X = np.array(sequences)\n",
    "y = np.array(labels)\n",
    "n_attack = y.sum()\n",
    "n_benign = (y == 0).sum()\n",
    "\n",
    "print(f\"\\n‚úÖ Built {len(X):,} sequences\")\n",
    "print(f\"   Attack: {n_attack:,} ({n_attack/len(y)*100:.1f}%)\")\n",
    "print(f\"   Benign: {n_benign:,} ({n_benign/len(y)*100:.1f}%)\")\n",
    "if n_attack < 50:\n",
    "    print(f\"‚ö†Ô∏è Only {n_attack} attack sequences. Consider lowering FAIL_THRESHOLD to 6.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: TRAIN LSTM\n",
    "# =============================================================================\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y if n_attack >= 10 else None\n",
    ")\n",
    "print(f\"Train: {len(X_train):,}, Test: {len(X_test):,}\")\n",
    "\n",
    "# Build model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=WINDOW_SIZE),\n",
    "    Bidirectional(LSTM(LSTM_UNITS, return_sequences=False)),\n",
    "    Dropout(DROPOUT),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(DROPOUT / 2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"\\nüß† Model architecture:\")\n",
    "model.summary()\n",
    "\n",
    "# Class weights for imbalance\n",
    "class_weight = {0: 1.0, 1: n_benign / n_attack} if n_attack > 0 and n_benign > 0 else None\n",
    "if class_weight:\n",
    "    print(f\"\\n‚öñÔ∏è Class weights: {class_weight}\")\n",
    "\n",
    "# Train\n",
    "print(f\"\\nüöÄ Training for up to {EPOCHS} epochs...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n",
    "print(\"\\n‚úÖ Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: EVALUATE\n",
    "# =============================================================================\n",
    "\n",
    "# Predict on test set\n",
    "y_prob = model.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "# Find optimal threshold\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "for thresh in np.arange(0.1, 0.9, 0.05):\n",
    "    y_pred_temp = (y_prob >= thresh).astype(int)\n",
    "    _, _, f1_temp, _ = precision_recall_fscore_support(y_test, y_pred_temp, average='binary', zero_division=0)\n",
    "    if f1_temp > best_f1:\n",
    "        best_f1 = f1_temp\n",
    "        best_threshold = thresh\n",
    "\n",
    "FINAL_THRESHOLD = best_threshold\n",
    "y_pred = (y_prob >= FINAL_THRESHOLD).astype(int)\n",
    "\n",
    "# Metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', zero_division=0)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOptimal Threshold: {FINAL_THRESHOLD:.2f}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   Recall:    {recall:.4f}\")\n",
    "print(f\"   F1 Score:  {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                  Predicted\")\n",
    "print(f\"                  Benign  Attack\")\n",
    "if cm.size == 4:\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "else:\n",
    "    tn, fp, fn, tp = cm[0,0], 0, 0, 0\n",
    "print(f\"Actual Benign     {tn:6}  {fp:6}\")\n",
    "print(f\"Actual Attack     {fn:6}  {tp:6}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: EXPORT ssh_lstm.joblib\n",
    "# =============================================================================\n",
    "from google.colab import files\n",
    "\n",
    "# Bundle everything needed to reload\n",
    "export_bundle = {\n",
    "    'model_json': model.to_json(),\n",
    "    'weights': model.get_weights(),\n",
    "    'token2id': token2id,\n",
    "    'window_size': WINDOW_SIZE,\n",
    "    'stride': STRIDE,\n",
    "    'fail_threshold': FAIL_THRESHOLD,\n",
    "    'time_window_sec': TIME_WINDOW_SEC,\n",
    "    'threshold': float(FINAL_THRESHOLD)\n",
    "}\n",
    "\n",
    "output_file = 'ssh_lstm.joblib'\n",
    "joblib.dump(export_bundle, output_file)\n",
    "print(f\"‚úÖ Saved: {output_file}\")\n",
    "\n",
    "# Download\n",
    "files.download(output_file)\n",
    "print(\"\\nüì• Download started!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: SUMMARY\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"üìã PIPELINE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Data:\")\n",
    "print(f\"   Parsed events:     {len(df_events):,}\")\n",
    "print(f\"   Unique IPs:        {df_events['src_ip'].nunique():,}\")\n",
    "print(f\"   Total sequences:   {len(X):,}\")\n",
    "print(f\"\\nüè∑Ô∏è Labels:\")\n",
    "print(f\"   Attack (1):  {n_attack:,} ({n_attack/len(y)*100:.1f}%)\")\n",
    "print(f\"   Benign (0):  {n_benign:,} ({n_benign/len(y)*100:.1f}%)\")\n",
    "print(f\"\\nüìà Model Performance:\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   Recall:    {recall:.4f}\")\n",
    "print(f\"   F1 Score:  {f1:.4f}\")\n",
    "print(f\"   Threshold: {FINAL_THRESHOLD:.2f}\")\n",
    "print(f\"\\nüì¶ Exported:\")\n",
    "print(f\"   File: {output_file}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚úÖ Pipeline complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
